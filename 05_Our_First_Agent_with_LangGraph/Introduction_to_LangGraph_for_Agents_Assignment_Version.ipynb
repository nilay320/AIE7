{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJXW_DgiSebM"
      },
      "source": [
        "# LangGraph and LangSmith - Agentic RAG Powered by LangChain\n",
        "\n",
        "In the following notebook we'll complete the following tasks:\n",
        "\n",
        "- ü§ù Breakout Room #1:\n",
        "  1. Install required libraries\n",
        "  2. Set Environment Variables\n",
        "  3. Creating our Tool Belt\n",
        "  4. Creating Our State\n",
        "  5. Creating and Compiling A Graph!\n",
        "\n",
        "- ü§ù Breakout Room #2:\n",
        "  1. Evaluating the LangGraph Application with LangSmith\n",
        "  2. Adding Helpfulness Check and \"Loop\" Limits\n",
        "  3. LangGraph for the \"Patterns\" of GenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djQ3nRAgoF67"
      },
      "source": [
        "# ü§ù Breakout Room #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7pQDUhUnIo8"
      },
      "source": [
        "## Part 1: LangGraph - Building Cyclic Applications with LangChain\n",
        "\n",
        "LangGraph is a tool that leverages LangChain Expression Language to build coordinated multi-actor and stateful applications that includes cyclic behaviour.\n",
        "\n",
        "### Why Cycles?\n",
        "\n",
        "In essence, we can think of a cycle in our graph as a more robust and customizable loop. It allows us to keep our application agent-forward while still giving the powerful functionality of traditional loops.\n",
        "\n",
        "Due to the inclusion of cycles over loops, we can also compose rather complex flows through our graph in a much more readable and natural fashion. Effectively allowing us to recreate application flowcharts in code in an almost 1-to-1 fashion.\n",
        "\n",
        "### Why LangGraph?\n",
        "\n",
        "Beyond the agent-forward approach - we can easily compose and combine traditional \"DAG\" (directed acyclic graph) chains with powerful cyclic behaviour due to the tight integration with LCEL. This means it's a natural extension to LangChain's core offerings!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_fLDElOVoop"
      },
      "source": [
        "## Task 1:  Dependencies\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wujPjGJuoPwg"
      },
      "source": [
        "## Task 2: Environment Variables\n",
        "\n",
        "We'll want to set both our OpenAI API key and our LangSmith environment variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jdh8CoVWHRvs",
        "outputId": "3fa78560-393c-4ee5-b871-9886bf0d70f4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jkla2fpx28QK",
        "outputId": "52d7ad22-fcb1-4abe-853b-216c55a12650"
      },
      "outputs": [],
      "source": [
        "os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"TAVILY_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nv0glIDyHmRt",
        "outputId": "b69df90a-b4e1-4ddb-9de0-882d98b68ab2"
      },
      "outputs": [],
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIE7 - LangGraph - {uuid4().hex[0:8]}\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"LangSmith API Key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBRyQmEAVzua"
      },
      "source": [
        "## Task 3: Creating our Tool Belt\n",
        "\n",
        "As is usually the case, we'll want to equip our agent with a toolbelt to help answer questions and add external knowledge.\n",
        "\n",
        "There's a tonne of tools in the [LangChain Community Repo](https://github.com/langchain-ai/langchain-community/tree/main/libs/community) but we'll stick to a couple just so we can observe the cyclic nature of LangGraph in action!\n",
        "\n",
        "We'll leverage:\n",
        "\n",
        "- [Tavily Search Results](https://github.com/langchain-ai/langchain-community/blob/main/libs/community/langchain_community/tools/tavily_search/tool.py)\n",
        "- [Arxiv](https://github.com/langchain-ai/langchain-community/blob/main/libs/community/langchain_community/tools/arxiv/tool.py)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2k6n_Dob2F46"
      },
      "source": [
        "#### üèóÔ∏è Activity #1:\n",
        "\n",
        "Please add the tools to use into our toolbelt.\n",
        "\n",
        "> NOTE: Each tool in our toolbelt should be a method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "lAxaSvlfIeOg"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/4x/mgh9b4ps5tn1vt9ylv195tx40000gn/T/ipykernel_60385/1203815797.py:4: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-tavily package and should be used instead. To use it run `pip install -U :class:`~langchain-tavily` and import as `from :class:`~langchain_tavily import TavilySearch``.\n",
            "  tavily_tool = TavilySearchResults(max_results=5)\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from langchain_community.tools.arxiv.tool import ArxivQueryRun\n",
        "\n",
        "tavily_tool = TavilySearchResults(max_results=5)\n",
        "\n",
        "tool_belt = [\n",
        "    tavily_tool,\n",
        "    ArxivQueryRun(),\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VI-C669ZYVI5"
      },
      "source": [
        "### Model\n",
        "\n",
        "Now we can set-up our model! We'll leverage the familiar OpenAI model suite for this example - but it's not *necessary* to use with LangGraph. LangGraph supports all models - though you might not find success with smaller models - as such, they recommend you stick with:\n",
        "\n",
        "- OpenAI's GPT-3.5 and GPT-4\n",
        "- Anthropic's Claude\n",
        "- Google's Gemini\n",
        "\n",
        "> NOTE: Because we're leveraging the OpenAI function calling API - we'll need to use OpenAI *for this specific example* (or any other service that exposes an OpenAI-style function calling API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "QkNS8rNZJs4z"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "#modelName = \"gpt-4.1-nano\"\n",
        "modelName = \"gpt-4o\"\n",
        "model = ChatOpenAI(model=modelName, temperature=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ugkj3GzuZpQv"
      },
      "source": [
        "Now that we have our model set-up, let's \"put on the tool belt\", which is to say: We'll bind our LangChain formatted tools to the model in an OpenAI function calling format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "4OdMqFafZ_0V"
      },
      "outputs": [],
      "source": [
        "model = model.bind_tools(tool_belt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERzuGo6W18Lr"
      },
      "source": [
        "#### ‚ùì Question #1:\n",
        "\n",
        "How does the model determine which tool to use?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### ‚úÖ Answer:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When you bind tools to a model, the model receives metadata about each tool, including:\n",
        "\n",
        "* Tool name and description\n",
        "* Parameter schemas\n",
        "* Use case examples\n",
        "\n",
        "The model uses this metadata to match the user's request to the appropriate tool based on semantic similarity and functional requirements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_296Ub96Z_H8"
      },
      "source": [
        "## Task 4: Putting the State in Stateful\n",
        "\n",
        "Earlier we used this phrasing:\n",
        "\n",
        "`coordinated multi-actor and stateful applications`\n",
        "\n",
        "So what does that \"stateful\" mean?\n",
        "\n",
        "To put it simply - we want to have some kind of object which we can pass around our application that holds information about what the current situation (state) is. Since our system will be constructed of many parts moving in a coordinated fashion - we want to be able to ensure we have some commonly understood idea of that state.\n",
        "\n",
        "LangGraph leverages a `StatefulGraph` which uses an `AgentState` object to pass information between the various nodes of the graph.\n",
        "\n",
        "There are more options than what we'll see below - but this `AgentState` object is one that is stored in a `TypedDict` with the key `messages` and the value is a `Sequence` of `BaseMessages` that will be appended to whenever the state changes.\n",
        "\n",
        "Let's think about a simple example to help understand exactly what this means (we'll simplify a great deal to try and clearly communicate what state is doing):\n",
        "\n",
        "1. We initialize our state object:\n",
        "  - `{\"messages\" : []}`\n",
        "2. Our user submits a query to our application.\n",
        "  - New State: `HumanMessage(#1)`\n",
        "  - `{\"messages\" : [HumanMessage(#1)}`\n",
        "3. We pass our state object to an Agent node which is able to read the current state. It will use the last `HumanMessage` as input. It gets some kind of output which it will add to the state.\n",
        "  - New State: `AgentMessage(#1, additional_kwargs {\"function_call\" : \"WebSearchTool\"})`\n",
        "  - `{\"messages\" : [HumanMessage(#1), AgentMessage(#1, ...)]}`\n",
        "4. We pass our state object to a \"conditional node\" (more on this later) which reads the last state to determine if we need to use a tool - which it can determine properly because of our provided object!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "mxL9b_NZKUdL"
      },
      "outputs": [],
      "source": [
        "from typing import TypedDict, Annotated\n",
        "from langgraph.graph.message import add_messages\n",
        "import operator\n",
        "from langchain_core.messages import BaseMessage\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "  messages: Annotated[list, add_messages]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWsMhfO9grLu"
      },
      "source": [
        "## Task 5: It's Graphing Time!\n",
        "\n",
        "Now that we have state, and we have tools, and we have an LLM - we can finally start making our graph!\n",
        "\n",
        "Let's take a second to refresh ourselves about what a graph is in this context.\n",
        "\n",
        "Graphs, also called networks in some circles, are a collection of connected objects.\n",
        "\n",
        "The objects in question are typically called nodes, or vertices, and the connections are called edges.\n",
        "\n",
        "Let's look at a simple graph.\n",
        "\n",
        "![image](https://i.imgur.com/2NFLnIc.png)\n",
        "\n",
        "Here, we're using the coloured circles to represent the nodes and the yellow lines to represent the edges. In this case, we're looking at a fully connected graph - where each node is connected by an edge to each other node.\n",
        "\n",
        "If we were to think about nodes in the context of LangGraph - we would think of a function, or an LCEL runnable.\n",
        "\n",
        "If we were to think about edges in the context of LangGraph - we might think of them as \"paths to take\" or \"where to pass our state object next\".\n",
        "\n",
        "Let's create some nodes and expand on our diagram.\n",
        "\n",
        "> NOTE: Due to the tight integration with LCEL - we can comfortably create our nodes in an async fashion!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "91flJWtZLUrl"
      },
      "outputs": [],
      "source": [
        "from langgraph.prebuilt import ToolNode\n",
        "\n",
        "def call_model(state):\n",
        "  messages = state[\"messages\"]\n",
        "  response = model.invoke(messages)\n",
        "  return {\"messages\" : [response]}\n",
        "\n",
        "tool_node = ToolNode(tool_belt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bwR7MgWj3Wg"
      },
      "source": [
        "Now we have two total nodes. We have:\n",
        "\n",
        "- `call_model` is a node that will...well...call the model\n",
        "- `tool_node` is a node which can call a tool\n",
        "\n",
        "Let's start adding nodes! We'll update our diagram along the way to keep track of what this looks like!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vF4_lgtmQNo",
        "outputId": "a4384377-8f7a-415f-be1b-fee6169cb101"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x131c57890>"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "uncompiled_graph = StateGraph(AgentState)\n",
        "\n",
        "uncompiled_graph.add_node(\"agent\", call_model)\n",
        "uncompiled_graph.add_node(\"action\", tool_node)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8CjRlbVmRpW"
      },
      "source": [
        "Let's look at what we have so far:\n",
        "\n",
        "![image](https://i.imgur.com/md7inqG.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaXHpPeSnOWC"
      },
      "source": [
        "Next, we'll add our entrypoint. All our entrypoint does is indicate which node is called first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGCbaYqRnmiw",
        "outputId": "5351807c-2ac7-4316-a3a3-878abeacd114"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x131c57890>"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "uncompiled_graph.set_entry_point(\"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUsfGoSpoF9U"
      },
      "source": [
        "![image](https://i.imgur.com/wNixpJe.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Q_pQgHmoW0M"
      },
      "source": [
        "Now we want to build a \"conditional edge\" which will use the output state of a node to determine which path to follow.\n",
        "\n",
        "We can help conceptualize this by thinking of our conditional edge as a conditional in a flowchart!\n",
        "\n",
        "Notice how our function simply checks if there is a \"function_call\" kwarg present.\n",
        "\n",
        "Then we create an edge where the origin node is our agent node and our destination node is *either* the action node or the END (finish the graph).\n",
        "\n",
        "It's important to highlight that the dictionary passed in as the third parameter (the mapping) should be created with the possible outputs of our conditional function in mind. In this case `should_continue` outputs either `\"end\"` or `\"continue\"` which are subsequently mapped to the action node or the END node."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BZgb81VQf9o",
        "outputId": "73a07c15-5f0b-40f2-b033-38b57d056dd8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x131c57890>"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def should_continue(state):\n",
        "  last_message = state[\"messages\"][-1]\n",
        "\n",
        "  if last_message.tool_calls:\n",
        "    return \"action\"\n",
        "\n",
        "  return END\n",
        "\n",
        "uncompiled_graph.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    should_continue\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Cvhcf4jp0Ce"
      },
      "source": [
        "Let's visualize what this looks like.\n",
        "\n",
        "![image](https://i.imgur.com/8ZNwKI5.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKCjWJCkrJb9"
      },
      "source": [
        "Finally, we can add our last edge which will connect our action node to our agent node. This is because we *always* want our action node (which is used to call our tools) to return its output to our agent!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvcgbHf1rIXZ",
        "outputId": "45d4bdd6-d6bb-4a1d-bb79-cad43c130bf2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x131c57890>"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "uncompiled_graph.add_edge(\"action\", \"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiWDwBQtrw7Z"
      },
      "source": [
        "Let's look at the final visualization.\n",
        "\n",
        "![image](https://i.imgur.com/NWO7usO.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYqDpErlsCsu"
      },
      "source": [
        "All that's left to do now is to compile our workflow - and we're off!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "zt9-KS8DpzNx"
      },
      "outputs": [],
      "source": [
        "simple_agent_graph = uncompiled_graph.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhNWIwBL1W4Q"
      },
      "source": [
        "#### ‚ùì Question #2:\n",
        "\n",
        "Is there any specific limit to how many times we can cycle?\n",
        "\n",
        "If not, how could we impose a limit to the number of cycles?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### ‚úÖ Answer:\n",
        "\n",
        "No, there is not a specific limit to how many times we can cycle.\n",
        "\n",
        "We can impose limits by:\n",
        "\n",
        "* Adding a counter to your state and checking the counter in should_continue\n",
        "* Implementing timeout mechanisms\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEYcTShCsPaa"
      },
      "source": [
        "## Using Our Graph\n",
        "\n",
        "Now that we've created and compiled our graph - we can call it *just as we'd call any other* `Runnable`!\n",
        "\n",
        "Let's try out a few examples to see how it fairs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qn4n37PQRPII",
        "outputId": "5eeedfae-089d-496e-e71f-071939fa5832"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_D0irAgnYl62Eeote3Bk9Zz0L', 'function': {'arguments': '{\"query\":\"current captain of the Winnipeg Jets\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 162, 'total_tokens': 185, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': None, 'id': 'chatcmpl-Bsc45nO52ByVsK6dA22zrST1StS7Z', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--195e34f6-f028-4e15-a910-0c311abf33b3-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'current captain of the Winnipeg Jets'}, 'id': 'call_D0irAgnYl62Eeote3Bk9Zz0L', 'type': 'tool_call'}], usage_metadata={'input_tokens': 162, 'output_tokens': 23, 'total_tokens': 185, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'action'\n",
            "[ToolMessage(content='[{\"title\": \"List of all the Winnipeg Jets Captains | Hockey-Reference.com\", \"url\": \"https://www.hockey-reference.com/teams/WPG/captains.html\", \"content\": \"| 2020-21 | Blake Wheeler | 34 | 50 | 15 | 31 | 46 | -17 | 50 |  |  |  |  |  |  |\\\\n| 2019-20 | Blake Wheeler | 33 | 71 | 22 | 43 | 65 | 1 | 37 |  |  |  |  |  |  |\\\\n| 2018-19 | Blake Wheeler | 32 | 82 | 20 | 71 | 91 | 0 | 60 |  |  |  |  |  |  |\\\\n| 2017-18 | Blake Wheeler | 31 | 81 | 23 | 68 | 91 | 13 | 52 |  |  |  |  |  |  |\\\\n| 2016-17 | Blake Wheeler | 30 | 82 | 26 | 48 | 74 | 6 | 47 |  |  |  |  |  |  |\\\\n| 2015-16 | Andrew Ladd | 30 | 59 | 17 | 17 | 34 | -10 | 39 |  |  |  |  |  |  | [...] | 2014-15 | Andrew Ladd | 29 | 81 | 24 | 38 | 62 | 9 | 72 |  |  |  |  |  |  |\\\\n| 2013-14 | Andrew Ladd | 28 | 78 | 23 | 31 | 54 | 8 | 57 |  |  |  |  |  |  |\\\\n| 2012-13 | Andrew Ladd | 27 | 48 | 18 | 28 | 46 | 10 | 22 |  |  |  |  |  |  |\\\\n| 2011-12 | Andrew Ladd | 26 | 82 | 28 | 22 | 50 | -8 | 64 |  |  |  |  |  |  |\\\\n| 2010-11 | Andrew Ladd | 25 | 81 | 29 | 30 | 59 | -10 | 39 |  |  |  |  |  |  |\\\\n| 2009-10 | Ilya Kovalchuk | 26 | 49 | 31 | 27 | 58 | 1 | 45 |  |  |  |  |  |  | [...] | 2008-09 | Ilya Kovalchuk | 25 | 79 | 43 | 48 | 91 | -12 | 50 |  |  |  |  |  |  |\\\\n| 2007-08 | Bobby Hol√É\\xadk | 37 | 82 | 15 | 19 | 34 | -14 | 90 |  |  |  |  |  |  |\\\\n| 2006-07 | Scott Mellanby | 40 | 69 | 12 | 24 | 36 | -9 | 63 |  |  |  |  |  |  |\\\\n| 2005-06 | Scott Mellanby | 39 | 71 | 12 | 22 | 34 | 5 | 55 |  |  |  |  |  |  |\\\\n| 2003-04 | Shawn McEachern | 34 | 82 | 17 | 38 | 55 | 5 | 76 |  |  |  |  |  |  |\\\\n| 2001-02 | Ray Ferraro | 37 | 61 | 8 | 19 | 27 | -32 | 66 |  |  |  |  |  |  |\", \"score\": 0.84948516}, {\"title\": \"Adam Lowry - Wikipedia\", \"url\": \"https://en.wikipedia.org/wiki/Adam_Lowry\", \"content\": \"| Awards and achievements | | |\\\\n| --- | --- | --- |\\\\n| Preceded by Stefan Elliott | Winner of the Daryl K. (Doc) Seaman Trophy_Seaman_Trophy \\\\\"Daryl K. (Doc) Seaman Trophy\\\\\")  2010 | Succeeded by Colin Smith \\\\\"Colin Smith (ice hockey)\\\\\") |\\\\n| Sporting positions | | |\\\\n| Preceded by Blake Wheeler | Winnipeg Jets captain  2023‚Äìpresent | Incumbent | [...] Adam Lowry (born March 29, 1993) is an American-born Canadian professional ice hockey centre \\\\\"Center (ice hockey)\\\\\") and  captain \\\\\"Captain (ice hockey)\\\\\") of the Winnipeg Jets of the National Hockey League (NHL).\\\\n\\\\n## Early life [...] Entering the 2023‚Äì24 season, Lowry was named captain of the Jets on September 12, 2023. He became the third in the team\\'s history since relocating to Winnipeg, and the tenth overall in franchise history.\\\\n\\\\nOn May 4, 2025, Lowry scored at 16:10 in double overtime to win 4-3 against the St. Louis Blues in Game 7 of the Jets-Blues first round playoff series.\\\\n\\\\n## Personal life\", \"score\": 0.8482825}, {\"title\": \"Adam Lowry named Jets captain | Winnipeg Jets - NHL.com\", \"url\": \"https://www.nhl.com/jets/news/adam-lowry-named-jets-captain\", \"content\": \"That honour was given to Winnipeg Jet forward Adam Lowry officially Tuesday morning as he becomes the third captain in franchise history since the team moved here from Atlanta. He follows Andrew Ladd and Blake Wheeler who served as captain for five and six years respectively.\\\\n\\\\n√¢\\x80\\x9cWhen I found out, I was pretty excited, almost a little speechless. It√¢\\x80\\x99s something growing up you kind of can dream about and something that seems almost unattainable,√¢\\x80\\x9d said Lowry. [...] √¢\\x80\\x9cHe√¢\\x80\\x99s a true professional, he has total respect from every player on the team, every player around the league and certainly from the coaching staff as well. We just feel at this point it√¢\\x80\\x99s the right time to name Adam as our captain.√¢\\x80\\x9d [...] √¢\\x80\\x9cGetting to be a captain of a Canadian NHL team is pretty special and something I√¢\\x80\\x99m really looking forward too.√¢\\x80\\x9d\", \"score\": 0.8449346}, {\"title\": \"Lowry named Jets captain, replaces Wheeler | NHL.com\", \"url\": \"https://www.nhl.com/news/adam-lowry-named-winnipeg-captain\", \"content\": \"NHL logo\\\\nNHL logo\\\\n\\\\n# Lowry named Jets captain, replaces Wheeler\\\\n\\\\n30-year-old forward entering 10th season with Winnipeg\\\\n\\\\nLowry_Jets\\\\n\\\\nAdam Lowry was named captain of the Winnipeg Jets on Tuesday.\\\\n\\\\nThe 30-year-old forward was selected by the Jets in the third round (No. 67) of the 2011 NHL Draft and has played his entire nine-season NHL career with Winnipeg. [...] Lowry replaces Blake Wheeler, who was removed as captain Sept. 16, 2022, and signed with the New York Rangers after having his contract bought out this offseason. The Jets opted for three alternate captains last season; Lowry, forward Mark Scheifele and defenseman Josh Morrissey. Coach Rick Bowness said Scheifele and Morrissey will remain alternate captains. [...] Lowry said he\\'s learned from the captains he\\'s played with in Winnipeg, Wheeler and Andrew Ladd, and believes the important thing is staying true to the player he\\'s always been.\", \"score\": 0.84027237}, {\"title\": \"Winnipeg Jets Captain To Miss Start of Next Season - Sports Illustrated\", \"url\": \"https://www.si.com/onsi/breakaway/news-feed-page/winnipeg-jets-adam-lowry-miss-start-next-season\", \"content\": \"The Winnipeg Jets fell short of expectations after winning the 2024-25 President√¢\\x80\\x99s Trophy, but they are retooling and looking forward to next season. Despite the high hopes, the Jets will likely have to start the 2025-26 season without their captain.\\\\n\\\\nThe Jets announced that captain Adam Lowry underwent a successful hip surgery and is expected to be sidelined for five to six months. Lowry played 73 games during the 2024-25 regular season and didn√¢\\x80\\x99t miss a single playoff game. [...] The following is a statement from the Winnipeg Jets Hockey Club regarding Captain Adam Lowry: pic.twitter.com/t6S7wajrKq\\\\n\\\\nLowry never showed signs of needing postseason surgery, averaging almost 17:30 in ice time throughout the playoffs.\\\\n\\\\nIn 13 playoff games this season, Lowry scored four goals, including the double overtime game-winner in Game 7 of their opening-round series against the St. Louis Blues.\", \"score\": 0.8361405}]', name='tavily_search_results_json', id='f980d72f-b067-4e4b-b369-b2d75b97439b', tool_call_id='call_D0irAgnYl62Eeote3Bk9Zz0L', artifact={'query': 'current captain of the Winnipeg Jets', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://www.hockey-reference.com/teams/WPG/captains.html', 'title': 'List of all the Winnipeg Jets Captains | Hockey-Reference.com', 'content': '| 2020-21 | Blake Wheeler | 34 | 50 | 15 | 31 | 46 | -17 | 50 |  |  |  |  |  |  |\\n| 2019-20 | Blake Wheeler | 33 | 71 | 22 | 43 | 65 | 1 | 37 |  |  |  |  |  |  |\\n| 2018-19 | Blake Wheeler | 32 | 82 | 20 | 71 | 91 | 0 | 60 |  |  |  |  |  |  |\\n| 2017-18 | Blake Wheeler | 31 | 81 | 23 | 68 | 91 | 13 | 52 |  |  |  |  |  |  |\\n| 2016-17 | Blake Wheeler | 30 | 82 | 26 | 48 | 74 | 6 | 47 |  |  |  |  |  |  |\\n| 2015-16 | Andrew Ladd | 30 | 59 | 17 | 17 | 34 | -10 | 39 |  |  |  |  |  |  | [...] | 2014-15 | Andrew Ladd | 29 | 81 | 24 | 38 | 62 | 9 | 72 |  |  |  |  |  |  |\\n| 2013-14 | Andrew Ladd | 28 | 78 | 23 | 31 | 54 | 8 | 57 |  |  |  |  |  |  |\\n| 2012-13 | Andrew Ladd | 27 | 48 | 18 | 28 | 46 | 10 | 22 |  |  |  |  |  |  |\\n| 2011-12 | Andrew Ladd | 26 | 82 | 28 | 22 | 50 | -8 | 64 |  |  |  |  |  |  |\\n| 2010-11 | Andrew Ladd | 25 | 81 | 29 | 30 | 59 | -10 | 39 |  |  |  |  |  |  |\\n| 2009-10 | Ilya Kovalchuk | 26 | 49 | 31 | 27 | 58 | 1 | 45 |  |  |  |  |  |  | [...] | 2008-09 | Ilya Kovalchuk | 25 | 79 | 43 | 48 | 91 | -12 | 50 |  |  |  |  |  |  |\\n| 2007-08 | Bobby Hol√É\\xadk | 37 | 82 | 15 | 19 | 34 | -14 | 90 |  |  |  |  |  |  |\\n| 2006-07 | Scott Mellanby | 40 | 69 | 12 | 24 | 36 | -9 | 63 |  |  |  |  |  |  |\\n| 2005-06 | Scott Mellanby | 39 | 71 | 12 | 22 | 34 | 5 | 55 |  |  |  |  |  |  |\\n| 2003-04 | Shawn McEachern | 34 | 82 | 17 | 38 | 55 | 5 | 76 |  |  |  |  |  |  |\\n| 2001-02 | Ray Ferraro | 37 | 61 | 8 | 19 | 27 | -32 | 66 |  |  |  |  |  |  |', 'score': 0.84948516, 'raw_content': None}, {'url': 'https://en.wikipedia.org/wiki/Adam_Lowry', 'title': 'Adam Lowry - Wikipedia', 'content': '| Awards and achievements | | |\\n| --- | --- | --- |\\n| Preceded by Stefan Elliott | Winner of the Daryl K. (Doc) Seaman Trophy_Seaman_Trophy \"Daryl K. (Doc) Seaman Trophy\")  2010 | Succeeded by Colin Smith \"Colin Smith (ice hockey)\") |\\n| Sporting positions | | |\\n| Preceded by Blake Wheeler | Winnipeg Jets captain  2023‚Äìpresent | Incumbent | [...] Adam Lowry (born March 29, 1993) is an American-born Canadian professional ice hockey centre \"Center (ice hockey)\") and  captain \"Captain (ice hockey)\") of the Winnipeg Jets of the National Hockey League (NHL).\\n\\n## Early life [...] Entering the 2023‚Äì24 season, Lowry was named captain of the Jets on September 12, 2023. He became the third in the team\\'s history since relocating to Winnipeg, and the tenth overall in franchise history.\\n\\nOn May 4, 2025, Lowry scored at 16:10 in double overtime to win 4-3 against the St. Louis Blues in Game 7 of the Jets-Blues first round playoff series.\\n\\n## Personal life', 'score': 0.8482825, 'raw_content': None}, {'url': 'https://www.nhl.com/jets/news/adam-lowry-named-jets-captain', 'title': 'Adam Lowry named Jets captain | Winnipeg Jets - NHL.com', 'content': 'That honour was given to Winnipeg Jet forward Adam Lowry officially Tuesday morning as he becomes the third captain in franchise history since the team moved here from Atlanta. He follows Andrew Ladd and Blake Wheeler who served as captain for five and six years respectively.\\n\\n√¢\\x80\\x9cWhen I found out, I was pretty excited, almost a little speechless. It√¢\\x80\\x99s something growing up you kind of can dream about and something that seems almost unattainable,√¢\\x80\\x9d said Lowry. [...] √¢\\x80\\x9cHe√¢\\x80\\x99s a true professional, he has total respect from every player on the team, every player around the league and certainly from the coaching staff as well. We just feel at this point it√¢\\x80\\x99s the right time to name Adam as our captain.√¢\\x80\\x9d [...] √¢\\x80\\x9cGetting to be a captain of a Canadian NHL team is pretty special and something I√¢\\x80\\x99m really looking forward too.√¢\\x80\\x9d', 'score': 0.8449346, 'raw_content': None}, {'url': 'https://www.nhl.com/news/adam-lowry-named-winnipeg-captain', 'title': 'Lowry named Jets captain, replaces Wheeler | NHL.com', 'content': \"NHL logo\\nNHL logo\\n\\n# Lowry named Jets captain, replaces Wheeler\\n\\n30-year-old forward entering 10th season with Winnipeg\\n\\nLowry_Jets\\n\\nAdam Lowry was named captain of the Winnipeg Jets on Tuesday.\\n\\nThe 30-year-old forward was selected by the Jets in the third round (No. 67) of the 2011 NHL Draft and has played his entire nine-season NHL career with Winnipeg. [...] Lowry replaces Blake Wheeler, who was removed as captain Sept. 16, 2022, and signed with the New York Rangers after having his contract bought out this offseason. The Jets opted for three alternate captains last season; Lowry, forward Mark Scheifele and defenseman Josh Morrissey. Coach Rick Bowness said Scheifele and Morrissey will remain alternate captains. [...] Lowry said he's learned from the captains he's played with in Winnipeg, Wheeler and Andrew Ladd, and believes the important thing is staying true to the player he's always been.\", 'score': 0.84027237, 'raw_content': None}, {'url': 'https://www.si.com/onsi/breakaway/news-feed-page/winnipeg-jets-adam-lowry-miss-start-next-season', 'title': 'Winnipeg Jets Captain To Miss Start of Next Season - Sports Illustrated', 'content': 'The Winnipeg Jets fell short of expectations after winning the 2024-25 President√¢\\x80\\x99s Trophy, but they are retooling and looking forward to next season. Despite the high hopes, the Jets will likely have to start the 2025-26 season without their captain.\\n\\nThe Jets announced that captain Adam Lowry underwent a successful hip surgery and is expected to be sidelined for five to six months. Lowry played 73 games during the 2024-25 regular season and didn√¢\\x80\\x99t miss a single playoff game. [...] The following is a statement from the Winnipeg Jets Hockey Club regarding Captain Adam Lowry: pic.twitter.com/t6S7wajrKq\\n\\nLowry never showed signs of needing postseason surgery, averaging almost 17:30 in ice time throughout the playoffs.\\n\\nIn 13 playoff games this season, Lowry scored four goals, including the double overtime game-winner in Game 7 of their opening-round series against the St. Louis Blues.', 'score': 0.8361405, 'raw_content': None}], 'response_time': 3.78})]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='The current captain of the Winnipeg Jets is Adam Lowry.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 2166, 'total_tokens': 2179, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': None, 'id': 'chatcmpl-Bsc4BloNMGdKPXKTnQqxezmxhiA97', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--9446d4b4-35ae-4796-8b1d-a3f97ec2dd80-0', usage_metadata={'input_tokens': 2166, 'output_tokens': 13, 'total_tokens': 2179, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "inputs = {\"messages\" : [HumanMessage(content=\"Who is the current captain of the Winnipeg Jets?\")]}\n",
        "\n",
        "async for chunk in simple_agent_graph.astream(inputs, stream_mode=\"updates\"):\n",
        "    for node, values in chunk.items():\n",
        "        print(f\"Receiving update from node: '{node}'\")\n",
        "        print(values[\"messages\"])\n",
        "        print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBHnUtLSscRr"
      },
      "source": [
        "Let's look at what happened:\n",
        "\n",
        "1. Our state object was populated with our request\n",
        "2. The state object was passed into our entry point (agent node) and the agent node added an `AIMessage` to the state object and passed it along the conditional edge\n",
        "3. The conditional edge received the state object, found the \"tool_calls\" `additional_kwarg`, and sent the state object to the action node\n",
        "4. The action node added the response from the OpenAI function calling endpoint to the state object and passed it along the edge to the agent node\n",
        "5. The agent node added a response to the state object and passed it along the conditional edge\n",
        "6. The conditional edge received the state object, could not find the \"tool_calls\" `additional_kwarg` and passed the state object to END where we see it output in the cell above!\n",
        "\n",
        "Now let's look at an example that shows a multiple tool usage - all with the same flow!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afv2BuEsV5JG",
        "outputId": "ff009536-d281-4a56-c126-9cd245352bfe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_Z0szZ162M8o0ngF5IpiKyb5h', 'function': {'arguments': '{\"query\":\"QLoRA\"}', 'name': 'arxiv'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 178, 'total_tokens': 194, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a288987b44', 'id': 'chatcmpl-Bsc7v4csXzysMNDiivLEJkazmnPpJ', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--21e40358-5e6c-4d2e-8584-c5e5f241762a-0', tool_calls=[{'name': 'arxiv', 'args': {'query': 'QLoRA'}, 'id': 'call_Z0szZ162M8o0ngF5IpiKyb5h', 'type': 'tool_call'}], usage_metadata={'input_tokens': 178, 'output_tokens': 16, 'total_tokens': 194, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'action'\n",
            "Tool Used: arxiv\n",
            "[ToolMessage(content='Published: 2023-05-23\\nTitle: QLoRA: Efficient Finetuning of Quantized LLMs\\nAuthors: Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer\\nSummary: We present QLoRA, an efficient finetuning approach that reduces memory usage\\nenough to finetune a 65B parameter model on a single 48GB GPU while preserving\\nfull 16-bit finetuning task performance. QLoRA backpropagates gradients through\\na frozen, 4-bit quantized pretrained language model into Low Rank\\nAdapters~(LoRA). Our best model family, which we name Guanaco, outperforms all\\nprevious openly released models on the Vicuna benchmark, reaching 99.3% of the\\nperformance level of ChatGPT while only requiring 24 hours of finetuning on a\\nsingle GPU. QLoRA introduces a number of innovations to save memory without\\nsacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is\\ninformation theoretically optimal for normally distributed weights (b) double\\nquantization to reduce the average memory footprint by quantizing the\\nquantization constants, and (c) paged optimziers to manage memory spikes. We\\nuse QLoRA to finetune more than 1,000 models, providing a detailed analysis of\\ninstruction following and chatbot performance across 8 instruction datasets,\\nmultiple model types (LLaMA, T5), and model scales that would be infeasible to\\nrun with regular finetuning (e.g. 33B and 65B parameter models). Our results\\nshow that QLoRA finetuning on a small high-quality dataset leads to\\nstate-of-the-art results, even when using smaller models than the previous\\nSoTA. We provide a detailed analysis of chatbot performance based on both human\\nand GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable\\nalternative to human evaluation. Furthermore, we find that current chatbot\\nbenchmarks are not trustworthy to accurately evaluate the performance levels of\\nchatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to\\nChatGPT. We release all of our models and code, including CUDA kernels for\\n4-bit training.\\n\\nPublished: 2024-05-27\\nTitle: Accurate LoRA-Finetuning Quantization of LLMs via Information Retention\\nAuthors: Haotong Qin, Xudong Ma, Xingyu Zheng, Xiaoyang Li, Yang Zhang, Shouda Liu, Jie Luo, Xianglong Liu, Michele Magno\\nSummary: The LoRA-finetuning quantization of LLMs has been extensively studied to\\nobtain accurate yet compact LLMs for deployment on resource-constrained\\nhardware. However, existing methods cause the quantized LLM to severely degrade\\nand even fail to benefit from the finetuning of LoRA. This paper proposes a\\nnovel IR-QLoRA for pushing quantized LLMs with LoRA to be highly accurate\\nthrough information retention. The proposed IR-QLoRA mainly relies on two\\ntechnologies derived from the perspective of unified information: (1)\\nstatistics-based Information Calibration Quantization allows the quantized\\nparameters of LLM to retain original information accurately; (2)\\nfinetuning-based Information Elastic Connection makes LoRA utilizes elastic\\nrepresentation transformation with diverse information. Comprehensive\\nexperiments show that IR-QLoRA can significantly improve accuracy across LLaMA\\nand LLaMA2 families under 2-4 bit-widths, e.g., 4- bit LLaMA-7B achieves 1.4%\\nimprovement on MMLU compared with the state-of-the-art methods. The significant\\nperformance gain requires only a tiny 0.31% additional time consumption,\\nrevealing the satisfactory efficiency of our IR-QLoRA. We highlight that\\nIR-QLoRA enjoys excellent versatility, compatible with various frameworks\\n(e.g., NormalFloat and Integer quantization) and brings general accuracy gains.\\nThe code is available at https://github.com/htqin/ir-qlora.\\n\\nPublished: 2025-02-05\\nTitle: Resource-Efficient & Effective Code Summarization\\nAuthors: Saima Afrin, Joseph Call, Khai-Nguyen Nguyen, Oscar Chaparro, Antonio Mastropaolo\\nSummary: Code Language Models (CLMs) have demonstrated high effectiveness in\\nautomating software engineering tasks such as bug fixing, code generation, and\\ncode documentation. This ', name='arxiv', id='9296afa4-3e64-4840-bcd1-25f62027e29e', tool_call_id='call_Z0szZ162M8o0ngF5IpiKyb5h')]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_RMuxiw6BQv524Q2Smf5dQmDG', 'function': {'arguments': '{\"query\": \"Tim Dettmers latest tweet\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}, {'id': 'call_zRBrGPoHFyPcLIspN4eQ4kds', 'function': {'arguments': '{\"query\": \"Artidoro Pagnoni latest tweet\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}, {'id': 'call_xzVtRMWsvEZuoU2JzFEYASpx', 'function': {'arguments': '{\"query\": \"Ari Holtzman latest tweet\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}, {'id': 'call_Eu7u80BGpx2JF3U1L9oFktEZ', 'function': {'arguments': '{\"query\": \"Luke Zettlemoyer latest tweet\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 112, 'prompt_tokens': 1148, 'total_tokens': 1260, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a288987b44', 'id': 'chatcmpl-Bsc7wKPSrFmsk1u1EfqqB9FZUljDS', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--2f1dea08-a432-41ce-9f8f-9251460fc2fb-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'Tim Dettmers latest tweet'}, 'id': 'call_RMuxiw6BQv524Q2Smf5dQmDG', 'type': 'tool_call'}, {'name': 'tavily_search_results_json', 'args': {'query': 'Artidoro Pagnoni latest tweet'}, 'id': 'call_zRBrGPoHFyPcLIspN4eQ4kds', 'type': 'tool_call'}, {'name': 'tavily_search_results_json', 'args': {'query': 'Ari Holtzman latest tweet'}, 'id': 'call_xzVtRMWsvEZuoU2JzFEYASpx', 'type': 'tool_call'}, {'name': 'tavily_search_results_json', 'args': {'query': 'Luke Zettlemoyer latest tweet'}, 'id': 'call_Eu7u80BGpx2JF3U1L9oFktEZ', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1148, 'output_tokens': 112, 'total_tokens': 1260, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'action'\n",
            "Tool Used: tavily_search_results_json\n",
            "[ToolMessage(content='[{\"title\": \"Tim Dettmers ‚Äî Making deep learning accessible.\", \"url\": \"https://timdettmers.com/\", \"content\": \"Filed Under: Academia, PhD Life Tagged With: Advisors, Grad school, PhD\\\\n\\\\n## TPUs vs GPUs for Transformers (BERT)\\\\n\\\\n2018-10-17 by Tim Dettmers 26 Comments [...] Filed Under: Deep Learning, Hardware Tagged With: AMD, CPU, High Performance Computing, Matrix Multiplication, Parallel Computing, PCIe Lanes, Sparse Training\\\\n\\\\n## LLM.int8() and Emergent Features\\\\n\\\\n2022-08-17 by Tim Dettmers 13 Comments [...] From that, I learned that quantization research is like printers. Nobody cares about printers. Nobody likes printers. But everybody is happy if printers do their job.\\\\n\\\\nFiled Under: Deep Learning Tagged With: emergent features, LLM.int8()\\\\n\\\\n## How to Choose Your Grad School\\\\n\\\\n2022-03-13 by Tim Dettmers 18 Comments\", \"score\": 0.6554468}, {\"title\": \"Tim Dettmers (@Tim_Dettmers) / X\", \"url\": \"https://twitter.com/Tim_Dettmers\", \"content\": \"Huge thank you to @NVIDIADC for gifting a brand new #NVIDIADGX B200 to CMU\\'s Catalyst Research Group! This AI supercomputing system will afford Catalyst the\", \"score\": 0.6479992}, {\"title\": \"Tim Dettmers (@Tim_Dettmers) / X\", \"url\": \"https://x.com/tim_dettmers\", \"content\": \"Exciting updates from #MLSys2025! All session recordings are now available and free to watch at https://mlsys.org. We\\'re also thrilled to announce that\", \"score\": 0.5533369}, {\"title\": \"Tim Dettmers - X\", \"url\": \"https://x.com/Tim_Dettmers/status/1876639222305878092\", \"content\": \"Tim Dettmers ¬∑ @Tim_Dettmers. I open Twitter, and I see this I believe we will get open-source AGI soon, but also that applying AGI will\", \"score\": 0.48703963}, {\"title\": \"About Me - Tim Dettmers\", \"url\": \"https://timdettmers.com/about/\", \"content\": \"2021                 NeurIPS 2021 Best Reviewer Award\\\\n\\\\n2018/2019      Jeff Dean ‚Äì Heidi Hopper Endowed Regental Fellowship\\\\n\\\\n2016/2017      Google Scholarship\\\\n\\\\n## Service\\\\n\\\\nReviewing:\\\\n\\\\n## Primary Sidebar\\\\n\\\\n### What to read next:\\\\n\\\\n### Recent Posts\\\\n\\\\n## Secondary Sidebar\\\\n\\\\nCopyright ¬© 2024 ¬∑ Genesis Framework ¬∑ WordPress ¬∑ Log in\\\\n\\\\n## \\\\n\\\\n## \\\\n\\\\n### [...] Convolutional 2D Knowledge Graph Embeddings, Tim Dettmers, Pasquale Minervini, Pontus Stenetorp, Sebastian Riedel. AAAI2018. [arXiv] [bib] [code] [data] [Q&A]\\\\n\\\\n#### 2016\\\\n\\\\n8-Bit Approximations for Parallelism in Deep Learning, Tim Dettmers. ICLR2016. [arXiv] [bib] [code] [data]\\\\n\\\\n## Awards & Honors\\\\n\\\\n2023                 Madrona Prize\\\\n\\\\n2023                 Google Open Source Award\\\\n\\\\n2023                 PyTorch Foundation Award\\\\n\\\\n2023                 Martin & Beate Block Award [...] I have a PhD from University of Washington advised by Luke Zettlemoyer working on efficient deep learning at the intersection between machine learning, natural language processing, and computer systems with a focus on quantization and sparsity. My main research goal is to empower everyone to make AI their own. I do this by making large models accessible through my research (QLoRA, LLM.int8(), k-bit inference scaling laws, Petals, SWARM) and by developing software that makes it easy to use my\", \"score\": 0.42835352}]', name='tavily_search_results_json', id='9aa8ea72-f5ea-42fe-bfc3-0108529e3a91', tool_call_id='call_RMuxiw6BQv524Q2Smf5dQmDG', artifact={'query': 'Tim Dettmers latest tweet', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://timdettmers.com/', 'title': 'Tim Dettmers ‚Äî Making deep learning accessible.', 'content': 'Filed Under: Academia, PhD Life Tagged With: Advisors, Grad school, PhD\\n\\n## TPUs vs GPUs for Transformers (BERT)\\n\\n2018-10-17 by Tim Dettmers 26 Comments [...] Filed Under: Deep Learning, Hardware Tagged With: AMD, CPU, High Performance Computing, Matrix Multiplication, Parallel Computing, PCIe Lanes, Sparse Training\\n\\n## LLM.int8() and Emergent Features\\n\\n2022-08-17 by Tim Dettmers 13 Comments [...] From that, I learned that quantization research is like printers. Nobody cares about printers. Nobody likes printers. But everybody is happy if printers do their job.\\n\\nFiled Under: Deep Learning Tagged With: emergent features, LLM.int8()\\n\\n## How to Choose Your Grad School\\n\\n2022-03-13 by Tim Dettmers 18 Comments', 'score': 0.6554468, 'raw_content': None}, {'url': 'https://twitter.com/Tim_Dettmers', 'title': 'Tim Dettmers (@Tim_Dettmers) / X', 'content': \"Huge thank you to @NVIDIADC for gifting a brand new #NVIDIADGX B200 to CMU's Catalyst Research Group! This AI supercomputing system will afford Catalyst the\", 'score': 0.6479992, 'raw_content': None}, {'url': 'https://x.com/tim_dettmers', 'title': 'Tim Dettmers (@Tim_Dettmers) / X', 'content': \"Exciting updates from #MLSys2025! All session recordings are now available and free to watch at https://mlsys.org. We're also thrilled to announce that\", 'score': 0.5533369, 'raw_content': None}, {'url': 'https://x.com/Tim_Dettmers/status/1876639222305878092', 'title': 'Tim Dettmers - X', 'content': 'Tim Dettmers ¬∑ @Tim_Dettmers. I open Twitter, and I see this I believe we will get open-source AGI soon, but also that applying AGI will', 'score': 0.48703963, 'raw_content': None}, {'url': 'https://timdettmers.com/about/', 'title': 'About Me - Tim Dettmers', 'content': '2021                 NeurIPS 2021 Best Reviewer Award\\n\\n2018/2019      Jeff Dean ‚Äì Heidi Hopper Endowed Regental Fellowship\\n\\n2016/2017      Google Scholarship\\n\\n## Service\\n\\nReviewing:\\n\\n## Primary Sidebar\\n\\n### What to read next:\\n\\n### Recent Posts\\n\\n## Secondary Sidebar\\n\\nCopyright ¬© 2024 ¬∑ Genesis Framework ¬∑ WordPress ¬∑ Log in\\n\\n## \\n\\n## \\n\\n### [...] Convolutional 2D Knowledge Graph Embeddings, Tim Dettmers, Pasquale Minervini, Pontus Stenetorp, Sebastian Riedel. AAAI2018. [arXiv] [bib] [code] [data] [Q&A]\\n\\n#### 2016\\n\\n8-Bit Approximations for Parallelism in Deep Learning, Tim Dettmers. ICLR2016. [arXiv] [bib] [code] [data]\\n\\n## Awards & Honors\\n\\n2023                 Madrona Prize\\n\\n2023                 Google Open Source Award\\n\\n2023                 PyTorch Foundation Award\\n\\n2023                 Martin & Beate Block Award [...] I have a PhD from University of Washington advised by Luke Zettlemoyer working on efficient deep learning at the intersection between machine learning, natural language processing, and computer systems with a focus on quantization and sparsity. My main research goal is to empower everyone to make AI their own. I do this by making large models accessible through my research (QLoRA, LLM.int8(), k-bit inference scaling laws, Petals, SWARM) and by developing software that makes it easy to use my', 'score': 0.42835352, 'raw_content': None}], 'response_time': 3.56}), ToolMessage(content='[{\"title\": \"Artidoro Pagnoni (@ArtidoroPagnoni) / X\", \"url\": \"https://x.com/artidoropagnoni?lang=en\", \"content\": \"Introducing FlexOlmo, a new paradigm for language model training that enables the co-development of AI through data collaboration.\", \"score\": 0.6755297}, {\"title\": \"Artidoro Pagnoni: Ciao!\", \"url\": \"https://artidoro.github.io/\", \"content\": \"Artidoro Pagnoni\\\\n\\\\n### Artidoro Pagnoni\\\\n\\\\nPhD student in NLP at the University of Washington\\\\n\\\\n# Ciao!\\\\n\\\\nI am a final-year PhD student in Computer Science at the University of Washington, advised by Luke Zettlemoyer, and a visiting researcher at Meta. My research focuses on resource efficiency and improving LLM scaling trends. [...] I have recently developed the Byte Latent Transformer, a new architecture that efficiently learns from raw byte data unlocking a new scaling dimension and paving the path towards universal byte models. With QLoRA, I reduced finetuning memory requirements by 15x and showed how to approach ChatGPT 3.5 performance in 24h on a single GPU. [...] Previously, I have also worked on sythetic data augmentation for improved controllability of generation systems, investigated language models‚Äô reasoning and world modeling abilities, and evaluated their factual errors, as well as societal challenge associated with their use.\", \"score\": 0.6490677}, {\"title\": \"Highlights by Artidoro Pagnoni (@ArtidoroPagnoni) / X\", \"url\": \"https://twitter.com/ArtidoroPagnoni/highlights\", \"content\": \"Artidoro Pagnoni\\'s Highlights ... Introducing the Byte Latent Transformer (BLT) ‚Äì An LLM architecture that scales better than Llama 3 using byte-patches instead\", \"score\": 0.64719695}, {\"title\": \"Artidoro Pagnoni (@ArtidoroPagnoni) / X\", \"url\": \"https://twitter.com/ArtidoroPagnoni\", \"content\": \"Artidoro Pagnoni\\'s posts ... Introducing the Byte Latent Transformer (BLT) ‚Äì An LLM architecture that scales better than Llama 3 using byte-patches instead of\", \"score\": 0.6100127}, {\"title\": \"Artidoro Pagnoni - Google Scholar\", \"url\": \"https://scholar.google.com/citations?user=oLXBw0YAAAAJ&hl=en\", \"content\": \"New articles by this author. New citations to this author. New articles related to this author\\'s research. Email address for updates ... Artidoro Pagnoni.\", \"score\": 0.38297242}]', name='tavily_search_results_json', id='133e80e4-216a-4cf7-b8f0-bc358cf903de', tool_call_id='call_zRBrGPoHFyPcLIspN4eQ4kds', artifact={'query': 'Artidoro Pagnoni latest tweet', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://x.com/artidoropagnoni?lang=en', 'title': 'Artidoro Pagnoni (@ArtidoroPagnoni) / X', 'content': 'Introducing FlexOlmo, a new paradigm for language model training that enables the co-development of AI through data collaboration.', 'score': 0.6755297, 'raw_content': None}, {'url': 'https://artidoro.github.io/', 'title': 'Artidoro Pagnoni: Ciao!', 'content': 'Artidoro Pagnoni\\n\\n### Artidoro Pagnoni\\n\\nPhD student in NLP at the University of Washington\\n\\n# Ciao!\\n\\nI am a final-year PhD student in Computer Science at the University of Washington, advised by Luke Zettlemoyer, and a visiting researcher at Meta. My research focuses on resource efficiency and improving LLM scaling trends. [...] I have recently developed the Byte Latent Transformer, a new architecture that efficiently learns from raw byte data unlocking a new scaling dimension and paving the path towards universal byte models. With QLoRA, I reduced finetuning memory requirements by 15x and showed how to approach ChatGPT 3.5 performance in 24h on a single GPU. [...] Previously, I have also worked on sythetic data augmentation for improved controllability of generation systems, investigated language models‚Äô reasoning and world modeling abilities, and evaluated their factual errors, as well as societal challenge associated with their use.', 'score': 0.6490677, 'raw_content': None}, {'url': 'https://twitter.com/ArtidoroPagnoni/highlights', 'title': 'Highlights by Artidoro Pagnoni (@ArtidoroPagnoni) / X', 'content': \"Artidoro Pagnoni's Highlights ... Introducing the Byte Latent Transformer (BLT) ‚Äì An LLM architecture that scales better than Llama 3 using byte-patches instead\", 'score': 0.64719695, 'raw_content': None}, {'url': 'https://twitter.com/ArtidoroPagnoni', 'title': 'Artidoro Pagnoni (@ArtidoroPagnoni) / X', 'content': \"Artidoro Pagnoni's posts ... Introducing the Byte Latent Transformer (BLT) ‚Äì An LLM architecture that scales better than Llama 3 using byte-patches instead of\", 'score': 0.6100127, 'raw_content': None}, {'url': 'https://scholar.google.com/citations?user=oLXBw0YAAAAJ&hl=en', 'title': 'Artidoro Pagnoni - Google Scholar', 'content': \"New articles by this author. New citations to this author. New articles related to this author's research. Email address for updates ... Artidoro Pagnoni.\", 'score': 0.38297242, 'raw_content': None}], 'response_time': 2.39}), ToolMessage(content='[{\"title\": \"Ari Holtzman - X\", \"url\": \"https://x.com/universeinanegg/status/1943036107970453942\", \"content\": \"Ari Holtzman ¬∑ @universeinanegg. Some thought experiments on what if we keep ignoring prompt science: 1 ... 7:54 PM ¬∑ Jul 9, 2025. ¬∑. 54. Views.\", \"score\": 0.6867278}, {\"title\": \"Ari Holtzman on X: \\\\\"Was such a blast to write this with ...\", \"url\": \"https://twitter.com/universeinanegg/status/1943036113183998086\", \"content\": \"@ChenhaoTan ! And thank you to. @PeterWestTM. for many useful conversations as we were putting the argument together! 7:54 PM ¬∑ Jul 9, 2025. ¬∑. 65. Views.\", \"score\": 0.6337056}, {\"title\": \"Ari Holtzman ‚Äì Department of Computer Science\", \"url\": \"https://cs.uchicago.edu/people/ari-holtzman/\", \"content\": \"### Research\\\\n\\\\n### Systems, Architecture & Networking\\\\n\\\\n### Awards & Honors\\\\n\\\\n### Get Updates\\\\n\\\\n### Follow [...] ### Diversity @ UChicago CS\\\\n\\\\nAt UChicago CS, we welcome students of all backgrounds and identities.\\\\n\\\\n### Our BPC Plan\\\\n\\\\nFostering an inclusive environment where students from all backgrounds can achieve their highest potential.\\\\n\\\\n### News\\\\n\\\\n### Report from GlobusWorld 2025: Going Beyond Data\\\\n\\\\nheadshots\\\\n\\\\n### University of Chicago PhD Graduates Secure Tenure-Track Faculty Positions Amid a Competitive Job Market\\\\n\\\\ntext to 3d example [...] ### Democratizing Digital Graphics: An Undergrad‚Äôs Unlikely Path To Putting Agency of 3D-Generation in Users‚Äô Hands\\\\n\\\\n### Events\\\\n\\\\n### Video\\\\n\\\\nfuture of AI panelists\\\\n\\\\n### The Future of AI Panel: Alumni Weekend\\\\n\\\\n### Can we authenticate human creativity?\\\\n\\\\nheadshot\\\\n\\\\n### AI and the Future of Work Panel: Featuring Nick Feamster\\\\n\\\\nheadhsot\\\\nheadhsot\\\\n\\\\n# Ari Holtzman\", \"score\": 0.38699567}, {\"title\": \"Ari Holtzman\", \"url\": \"https://ariholtzman.com/\", \"content\": \"My primary interest is in generative models, how they work and how we can get them to generate text and other media that communicate with humans is useful and\", \"score\": 0.3413195}, {\"title\": \"Ari Holtzman on X: \\\\\"This is why I was largely anti-theory ...\", \"url\": \"https://twitter.com/universeinanegg/status/1849249175772463350\", \"content\": \"This is why I was largely anti-theory until grad school, and even now I have a much more coarse-to-fine ideology about theory building than\", \"score\": 0.25260106}]', name='tavily_search_results_json', id='2615894b-c811-4582-84c5-e600d54b6741', tool_call_id='call_xzVtRMWsvEZuoU2JzFEYASpx', artifact={'query': 'Ari Holtzman latest tweet', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://x.com/universeinanegg/status/1943036107970453942', 'title': 'Ari Holtzman - X', 'content': 'Ari Holtzman ¬∑ @universeinanegg. Some thought experiments on what if we keep ignoring prompt science: 1 ... 7:54 PM ¬∑ Jul 9, 2025. ¬∑. 54. Views.', 'score': 0.6867278, 'raw_content': None}, {'url': 'https://twitter.com/universeinanegg/status/1943036113183998086', 'title': 'Ari Holtzman on X: \"Was such a blast to write this with ...', 'content': '@ChenhaoTan ! And thank you to. @PeterWestTM. for many useful conversations as we were putting the argument together! 7:54 PM ¬∑ Jul 9, 2025. ¬∑. 65. Views.', 'score': 0.6337056, 'raw_content': None}, {'url': 'https://cs.uchicago.edu/people/ari-holtzman/', 'title': 'Ari Holtzman ‚Äì Department of Computer Science', 'content': '### Research\\n\\n### Systems, Architecture & Networking\\n\\n### Awards & Honors\\n\\n### Get Updates\\n\\n### Follow [...] ### Diversity @ UChicago CS\\n\\nAt UChicago CS, we welcome students of all backgrounds and identities.\\n\\n### Our BPC Plan\\n\\nFostering an inclusive environment where students from all backgrounds can achieve their highest potential.\\n\\n### News\\n\\n### Report from GlobusWorld 2025: Going Beyond Data\\n\\nheadshots\\n\\n### University of Chicago PhD Graduates Secure Tenure-Track Faculty Positions Amid a Competitive Job Market\\n\\ntext to 3d example [...] ### Democratizing Digital Graphics: An Undergrad‚Äôs Unlikely Path To Putting Agency of 3D-Generation in Users‚Äô Hands\\n\\n### Events\\n\\n### Video\\n\\nfuture of AI panelists\\n\\n### The Future of AI Panel: Alumni Weekend\\n\\n### Can we authenticate human creativity?\\n\\nheadshot\\n\\n### AI and the Future of Work Panel: Featuring Nick Feamster\\n\\nheadhsot\\nheadhsot\\n\\n# Ari Holtzman', 'score': 0.38699567, 'raw_content': None}, {'url': 'https://ariholtzman.com/', 'title': 'Ari Holtzman', 'content': 'My primary interest is in generative models, how they work and how we can get them to generate text and other media that communicate with humans is useful and', 'score': 0.3413195, 'raw_content': None}, {'url': 'https://twitter.com/universeinanegg/status/1849249175772463350', 'title': 'Ari Holtzman on X: \"This is why I was largely anti-theory ...', 'content': 'This is why I was largely anti-theory until grad school, and even now I have a much more coarse-to-fine ideology about theory building than', 'score': 0.25260106, 'raw_content': None}], 'response_time': 8.21}), ToolMessage(content='[{\"title\": \"Luke Zettlemoyer (@LukeZettlemoyer) / X\", \"url\": \"https://x.com/lukezettlemoyer?lang=en\", \"content\": \"Introducing FlexOlmo, a new paradigm for language model training that enables the co-development of AI through data collaboration.\", \"score\": 0.69498676}, {\"title\": \"Luke Zettlemoyer - AI at Meta\", \"url\": \"https://ai.meta.com/people/1450721039196474/luke-zettlemoyer/\", \"content\": \"October 18, 2019\\\\n\\\\n#### RESEARCH\\\\n\\\\n#### SPEECH & AUDIO\\\\n\\\\n#### A Discrete Hard EM Approach for Weakly Supervised Question Answering\\\\n\\\\nLuke Zettlemoyer, Danqi Chen, Hanna Hajishirzi, Sweon Min\\\\n\\\\nOctober 18, 2019\\\\n\\\\nFoundational models\\\\n\\\\nOur approach\\\\n\\\\nResearch\\\\n\\\\nMeta AI\\\\n\\\\nLatest news\\\\n\\\\nFoundational models\\\\n\\\\nMeta ¬© 2025 [...] Scott Yih, Luke Zettlemoyer, Mike Lewis, Hannaneh Hajishirzi, Kalpesh Krishna, Mohit Iyyer, Pang Wei Koh, Sewon Min, Xinxi Lyu\\\\n\\\\nNovember 17, 2023\\\\n\\\\nOctober 27, 2023\\\\n\\\\n#### CONVERSATIONAL AI\\\\n\\\\n#### NLP\\\\n\\\\n#### XLM-V: Overcoming the Vocabulary Bottleneck in Multilingual Masked Language Models\\\\n\\\\nDavis Liang, Hila Gonen, Yuning Mao, Rui Hou, Naman Goyal, Marjan Ghazvininejad, Luke Zettlemoyer, Madian Khabsa\\\\n\\\\nOctober 27, 2023\\\\n\\\\nSeptember 03, 2023\\\\n\\\\n#### NLP\\\\n\\\\n#### COMPUTER VISION [...] Rulin Shao, Qiao Rui, Varsha Kishore, Niklas Muennighoff, Victoria Lin, Daniela Rus, Bryan Kian Hsiang Low, Sewon Min, Scott Yih, Pang Wei Koh, Luke Zettlemoyer\\\\n\\\\nApril 25, 2025\\\\n\\\\nDecember 12, 2024\\\\n\\\\n#### NLP\\\\n\\\\n#### Byte Latent Transformer: Patches Scale Better Than Tokens\\\\n\\\\nArtidoro Pagnoni, Ram Pasunuru, Pedro Rodriguez, John Nguyen, Benjamin Muller, Margaret Li, Chunting Zhou, Lili Yu, Jason Weston, Luke Zettlemoyer, Gargi Ghosh, Mike Lewis, Ari Holtzman, Srini Iyer\\\\n\\\\nDecember 12, 2024\", \"score\": 0.65887916}, {\"title\": \"Luke Zettlemoyer (@LukeZettlemoyer) / X\", \"url\": \"https://mobile.x.com/LukeZettlemoyer\", \"content\": \"New paper from FAIR+NYU: Q: Is language supervision required to learn effective visual representations for multimodal tasks? A: No. ‚¨á.\", \"score\": 0.6255073}, {\"title\": \"Luke Zettlemoyer - X\", \"url\": \"https://x.com/LukeZettlemoyer/status/1003662931479941120?lang=ar-x-fm\", \"content\": \"Luke Zettlemoyer ¬∑ @LukeZettlemoyer. Come see Julian Michael presenting his work in question answer meaning representations, now at @NAACLHLT\", \"score\": 0.5530473}, {\"title\": \"[PDF] An Evaluation of Language Models for Hyper-partisan Ideology ...\", \"url\": \"https://aclanthology.org/2024.eurali-1.8.pdf\", \"content\": \"-Saturday, October 23, 2022 #Mahsa_Amini #IranRevoIution Government Supporters The chador (veil) you have put on is around the enemy‚Äôs neck. So hold your chador tighter! #Labbaik_Ya_Khamenei #End_of_Appeasement A student who was martyred due to knife attacks by street thugs and hooligans.\\\\n#Labbaik_Ya_Khamenei #End_Immorality Other Groups We will not back down because of the blood you shed and the children you imprisoned. [...] CoRR, abs/2005.14165.\\\\nWei Chen, Xiao Zhang, Tengjiao Wang, Bishan Yang, and Yi Li. 2017. Opinion-aware knowl-edge graph for political ideology detection. In International Joint Conference on Artificial Intel-ligence.\\\\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm√°n, √âdouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020. [...] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019a.\\\\nRoberta: A robustly opti-mized bert pretraining approach. arXiv preprint arXiv:1907.11692.\\\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.\\\\n2019b. Roberta: A robustly optimized BERT pretraining approach. CoRR, abs/1907.11692.\", \"score\": 0.30507636}]', name='tavily_search_results_json', id='8458411f-67dd-45d1-b5f4-c032a31ee216', tool_call_id='call_Eu7u80BGpx2JF3U1L9oFktEZ', artifact={'query': 'Luke Zettlemoyer latest tweet', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://x.com/lukezettlemoyer?lang=en', 'title': 'Luke Zettlemoyer (@LukeZettlemoyer) / X', 'content': 'Introducing FlexOlmo, a new paradigm for language model training that enables the co-development of AI through data collaboration.', 'score': 0.69498676, 'raw_content': None}, {'url': 'https://ai.meta.com/people/1450721039196474/luke-zettlemoyer/', 'title': 'Luke Zettlemoyer - AI at Meta', 'content': 'October 18, 2019\\n\\n#### RESEARCH\\n\\n#### SPEECH & AUDIO\\n\\n#### A Discrete Hard EM Approach for Weakly Supervised Question Answering\\n\\nLuke Zettlemoyer, Danqi Chen, Hanna Hajishirzi, Sweon Min\\n\\nOctober 18, 2019\\n\\nFoundational models\\n\\nOur approach\\n\\nResearch\\n\\nMeta AI\\n\\nLatest news\\n\\nFoundational models\\n\\nMeta ¬© 2025 [...] Scott Yih, Luke Zettlemoyer, Mike Lewis, Hannaneh Hajishirzi, Kalpesh Krishna, Mohit Iyyer, Pang Wei Koh, Sewon Min, Xinxi Lyu\\n\\nNovember 17, 2023\\n\\nOctober 27, 2023\\n\\n#### CONVERSATIONAL AI\\n\\n#### NLP\\n\\n#### XLM-V: Overcoming the Vocabulary Bottleneck in Multilingual Masked Language Models\\n\\nDavis Liang, Hila Gonen, Yuning Mao, Rui Hou, Naman Goyal, Marjan Ghazvininejad, Luke Zettlemoyer, Madian Khabsa\\n\\nOctober 27, 2023\\n\\nSeptember 03, 2023\\n\\n#### NLP\\n\\n#### COMPUTER VISION [...] Rulin Shao, Qiao Rui, Varsha Kishore, Niklas Muennighoff, Victoria Lin, Daniela Rus, Bryan Kian Hsiang Low, Sewon Min, Scott Yih, Pang Wei Koh, Luke Zettlemoyer\\n\\nApril 25, 2025\\n\\nDecember 12, 2024\\n\\n#### NLP\\n\\n#### Byte Latent Transformer: Patches Scale Better Than Tokens\\n\\nArtidoro Pagnoni, Ram Pasunuru, Pedro Rodriguez, John Nguyen, Benjamin Muller, Margaret Li, Chunting Zhou, Lili Yu, Jason Weston, Luke Zettlemoyer, Gargi Ghosh, Mike Lewis, Ari Holtzman, Srini Iyer\\n\\nDecember 12, 2024', 'score': 0.65887916, 'raw_content': None}, {'url': 'https://mobile.x.com/LukeZettlemoyer', 'title': 'Luke Zettlemoyer (@LukeZettlemoyer) / X', 'content': 'New paper from FAIR+NYU: Q: Is language supervision required to learn effective visual representations for multimodal tasks? A: No. ‚¨á.', 'score': 0.6255073, 'raw_content': None}, {'url': 'https://x.com/LukeZettlemoyer/status/1003662931479941120?lang=ar-x-fm', 'title': 'Luke Zettlemoyer - X', 'content': 'Luke Zettlemoyer ¬∑ @LukeZettlemoyer. Come see Julian Michael presenting his work in question answer meaning representations, now at @NAACLHLT', 'score': 0.5530473, 'raw_content': None}, {'url': 'https://aclanthology.org/2024.eurali-1.8.pdf', 'title': '[PDF] An Evaluation of Language Models for Hyper-partisan Ideology ...', 'content': '-Saturday, October 23, 2022 #Mahsa_Amini #IranRevoIution Government Supporters The chador (veil) you have put on is around the enemy‚Äôs neck. So hold your chador tighter! #Labbaik_Ya_Khamenei #End_of_Appeasement A student who was martyred due to knife attacks by street thugs and hooligans.\\n#Labbaik_Ya_Khamenei #End_Immorality Other Groups We will not back down because of the blood you shed and the children you imprisoned. [...] CoRR, abs/2005.14165.\\nWei Chen, Xiao Zhang, Tengjiao Wang, Bishan Yang, and Yi Li. 2017. Opinion-aware knowl-edge graph for political ideology detection. In International Joint Conference on Artificial Intel-ligence.\\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm√°n, √âdouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020. [...] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019a.\\nRoberta: A robustly opti-mized bert pretraining approach. arXiv preprint arXiv:1907.11692.\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.\\n2019b. Roberta: A robustly optimized BERT pretraining approach. CoRR, abs/1907.11692.', 'score': 0.30507636, 'raw_content': None}], 'response_time': 3.92})]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='Here are the latest tweets or updates from the authors of the QLoRA paper:\\n\\n1. **Tim Dettmers**: \\n   - [Latest Tweet](https://twitter.com/Tim_Dettmers): \"Huge thank you to @NVIDIADC for gifting a brand new #NVIDIADGX B200 to CMU\\'s Catalyst Research Group! This AI supercomputing system will afford Catalyst the...\"\\n\\n2. **Artidoro Pagnoni**:\\n   - [Latest Tweet](https://x.com/artidoropagnoni?lang=en): \"Introducing FlexOlmo, a new paradigm for language model training that enables the co-development of AI through data collaboration.\"\\n\\n3. **Ari Holtzman**:\\n   - [Latest Tweet](https://x.com/universeinanegg/status/1943036107970453942): \"Some thought experiments on what if we keep ignoring prompt science: 1 ...\"\\n\\n4. **Luke Zettlemoyer**:\\n   - [Latest Tweet](https://x.com/lukezettlemoyer?lang=en): \"Introducing FlexOlmo, a new paradigm for language model training that enables the co-development of AI through data collaboration.\"\\n\\nIt seems that both Artidoro Pagnoni and Luke Zettlemoyer have tweeted about the same topic, FlexOlmo, which is a new paradigm for language model training.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 282, 'prompt_tokens': 4613, 'total_tokens': 4895, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 1152}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a288987b44', 'id': 'chatcmpl-Bsc86n3CzlTMUuPauB568MFu1Mq5a', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--aa6815d1-b5fd-4494-8c4f-b7be5d181bf4-0', usage_metadata={'input_tokens': 4613, 'output_tokens': 282, 'total_tokens': 4895, 'input_token_details': {'audio': 0, 'cache_read': 1152}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "inputs = {\"messages\" : [HumanMessage(content=\"Search Arxiv for the QLoRA paper, then search each of the authors to find out their latest Tweet using Tavily!\")]}\n",
        "\n",
        "async for chunk in simple_agent_graph.astream(inputs, stream_mode=\"updates\"):\n",
        "    for node, values in chunk.items():\n",
        "        print(f\"Receiving update from node: '{node}'\")\n",
        "        if node == \"action\":\n",
        "          print(f\"Tool Used: {values['messages'][0].name}\")\n",
        "        print(values[\"messages\"])\n",
        "\n",
        "        print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXzDlZVz1Hnf"
      },
      "source": [
        "#### üèóÔ∏è Activity #2:\n",
        "\n",
        "Please write out the steps the agent took to arrive at the correct answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### ‚úÖ Answer:\n",
        "\n",
        "1. Agent Node (Cycle 1):\n",
        "\n",
        "Receives the complex query about searching Arxiv for QLoRA paper and finding authors' tweets.\n",
        "Decides to start with Arxiv search.\n",
        "Makes tool call: arxiv with query \"QLoRA\".\n",
        "\n",
        "\n",
        "2. Action Node (Cycle 1):\n",
        "\n",
        "Executes Arxiv search tool.\n",
        "Successfully retrieves QLoRA paper information including all four authors: Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer.\n",
        "\n",
        "\n",
        "3. Agent Node (Cycle 2):\n",
        "\n",
        "Processes the Arxiv results and extracts author names.\n",
        "Intelligently decides to search for ALL authors' tweets simultaneously.\n",
        "Makes 4 parallel tool calls to Tavily:\n",
        "\n",
        "\"Tim Dettmers latest tweet\", \n",
        "\"Artidoro Pagnoni latest tweet\", \n",
        "\"Ari Holtzman latest tweet\", \n",
        "\"Luke Zettlemoyer latest tweet\"\n",
        "\n",
        "\n",
        "4. Action Node (Cycle 2):\n",
        "\n",
        "Executes all 4 Tavily searches in parallel.\n",
        "Successfully finds tweet information for each author.\n",
        "\n",
        "\n",
        "5. Agent Node (Final):\n",
        "\n",
        "Synthesizes all the search results.\n",
        "Provides comprehensive final response with tweets from all 4 authors.\n",
        "Notes interesting connection (Pagnoni and Zettlemoyer both tweeting about FlexOlmo)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ü§ù Breakout Room #2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7c8-Uyarh1v"
      },
      "source": [
        "## Part 1: LangSmith Evaluator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pV3XeFOT1Sar"
      },
      "source": [
        "### Pre-processing for LangSmith"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wruQCuzewUuO"
      },
      "source": [
        "To do a little bit more preprocessing, let's wrap our LangGraph agent in a simple chain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "oeXdQgbxwhTv"
      },
      "outputs": [],
      "source": [
        "def convert_inputs(input_object):\n",
        "  return {\"messages\" : [HumanMessage(content=input_object[\"question\"])]}\n",
        "\n",
        "def parse_output(input_state):\n",
        "  return input_state[\"messages\"][-1].content\n",
        "\n",
        "agent_chain_with_formatting = convert_inputs | simple_agent_graph | parse_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "orYxBZXSxJjZ",
        "outputId": "76be837b-6424-4516-8f63-07fbd8c25bf5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"RAG stands for Retrieval-Augmented Generation. It is a technique used in natural language processing (NLP) that combines retrieval-based methods with generative models to improve the quality and accuracy of generated text. Here's how it generally works:\\n\\n1. **Retrieval**: The system first retrieves relevant information or documents from a large corpus or database. This step is crucial for providing context and factual grounding to the generative model.\\n\\n2. **Augmentation**: The retrieved information is then used to augment the input to a generative model. This can involve incorporating the retrieved data directly into the input or using it to inform the generation process in some way.\\n\\n3. **Generation**: Finally, a generative model, such as a transformer-based language model, uses the augmented input to produce a response or generate text. The inclusion of retrieved information helps ensure that the output is more accurate and contextually relevant.\\n\\nRAG is particularly useful in applications where the model needs to generate text that is both coherent and factually accurate, such as in question answering, summarization, and conversational agents. By leveraging external knowledge sources, RAG models can overcome some of the limitations of purely generative models, which may not have access to up-to-date or domain-specific information.\""
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent_chain_with_formatting.invoke({\"question\" : \"What is RAG?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9UkCIqkpyZu"
      },
      "source": [
        "### Task 1: Creating An Evaluation Dataset\n",
        "\n",
        "Just as we saw last week, we'll want to create a dataset to test our Agent's ability to answer questions.\n",
        "\n",
        "In order to do this - we'll want to provide some questions and some answers. Let's look at how we can create such a dataset below.\n",
        "\n",
        "```python\n",
        "questions = [\n",
        "    \"What optimizer is used in QLoRA?\",\n",
        "    \"What data type was created in the QLoRA paper?\",\n",
        "    \"What is a Retrieval Augmented Generation system?\",\n",
        "    \"Who authored the QLoRA paper?\",\n",
        "    \"What is the most popular deep learning framework?\",\n",
        "    \"What significant improvements does the LoRA system make?\"\n",
        "]\n",
        "\n",
        "answers = [\n",
        "    {\"must_mention\" : [\"paged\", \"optimizer\"]},\n",
        "    {\"must_mention\" : [\"NF4\", \"NormalFloat\"]},\n",
        "    {\"must_mention\" : [\"ground\", \"context\"]},\n",
        "    {\"must_mention\" : [\"Tim\", \"Dettmers\"]},\n",
        "    {\"must_mention\" : [\"PyTorch\", \"TensorFlow\"]},\n",
        "    {\"must_mention\" : [\"reduce\", \"parameters\"]},\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfMXF2KAsQxs"
      },
      "source": [
        "#### üèóÔ∏è Activity #3:\n",
        "\n",
        "Please create a dataset in the above format with at least 5 questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "CbagRuJop83E"
      },
      "outputs": [],
      "source": [
        "questions = [\n",
        "    \"Who founded Brazilian Jiu Jitsu and what family is most associated with its development?\",\n",
        "    \"What are the main belt ranks in Brazilian Jiu Jitsu from beginner to expert?\",\n",
        "    \"What is the difference between gi and no-gi Brazilian Jiu Jitsu?\",\n",
        "    \"Who is considered the most successful BJJ competitor in IBJJF World Championships history?\",\n",
        "    \"What does 'guard' mean in Brazilian Jiu Jitsu and name two types of guard positions?\",\n",
        "    \"What major BJJ competition is held annually in Las Vegas and who organizes it?\",\n",
        "    \"What is the philosophy behind Brazilian Jiu Jitsu regarding smaller practitioners vs larger opponents?\"\n",
        "]\n",
        "\n",
        "answers = [\n",
        "    {\"must_mention\": [\"Helio Gracie\", \"Gracie family\", \"Carlos Gracie\"]},\n",
        "    {\"must_mention\": [\"white\", \"blue\", \"purple\", \"brown\", \"black\"]},\n",
        "    {\"must_mention\": [\"kimono\", \"uniform\", \"gi\", \"without\"]},\n",
        "    {\"must_mention\": [\"Marcus Almeida\", \"Buchecha\", \"world championships\"]},\n",
        "    {\"must_mention\": [\"bottom position\", \"closed guard\", \"open guard\"]},\n",
        "    {\"must_mention\": [\"ADCC\", \"Abu Dhabi Combat Club\", \"Las Vegas\"]},\n",
        "    {\"must_mention\": [\"technique\", \"leverage\", \"smaller\", \"larger\"]}\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7QVFuAmsh7L"
      },
      "source": [
        "Now we can add our dataset to our LangSmith project using the following code which we saw last Thursday!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "RLfrZrgSsn85"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'example_ids': ['0a255b0c-9a21-473d-9d02-d2ce998bc4a9',\n",
              "  'f78eda6e-897a-49d2-bfcc-29b65ecec700',\n",
              "  '54f61cc1-39dd-4789-b908-0dbe89048045',\n",
              "  '25bc845b-255d-48a1-90ca-2a691dc411ef',\n",
              "  'c7e933f9-5d4d-4a4f-beab-fa61ec5a51df',\n",
              "  'ddb94618-2479-45ca-a6e8-33fefc0be2b5',\n",
              "  'a77f9a71-8f24-4adb-8ab1-2f03843fb85e'],\n",
              " 'count': 7}"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langsmith import Client\n",
        "\n",
        "client = Client()\n",
        "\n",
        "dataset_name = f\"Brazilian Jiu Jitsu - Agent Evaluation Dataset - {uuid4().hex[0:8]}\"\n",
        "\n",
        "dataset = client.create_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    description=\"Questions about Brazilian Jiu Jitsu to evaluate agent search and knowledge capabilities.\"\n",
        ")\n",
        "\n",
        "client.create_examples(\n",
        "    inputs=[{\"question\" : q} for q in questions],\n",
        "    outputs=answers,\n",
        "    dataset_id=dataset.id,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciV73F9Q04w0"
      },
      "source": [
        "#### ‚ùì Question #3:\n",
        "\n",
        "How are the correct answers associated with the questions?\n",
        "\n",
        "> NOTE: Feel free to indicate if this is problematic or not"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### ‚úÖ Answer:\n",
        "\n",
        "* The correct answers are associated with the questions positionally, i.e. the first question in the questions array is related to the first answer in the answers array.\n",
        "* This is problematic because it can lead to editing errors where:\n",
        "  - A question is added to the questions array but not the answers array\n",
        "  - An answer is added in the wrong position in the answers array  \n",
        "  - Lists become different lengths, causing silent mismatches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lRTXUrTtP9Y"
      },
      "source": [
        "### Task 2: Adding Evaluators\n",
        "\n",
        "Now we can add a custom evaluator to see if our responses contain the expected information.\n",
        "\n",
        "We'll be using a fairly naive exact-match process to determine if our response contains specific strings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "QrAUXMFftlAY"
      },
      "outputs": [],
      "source": [
        "from langsmith.evaluation import EvaluationResult, run_evaluator\n",
        "\n",
        "@run_evaluator\n",
        "def must_mention(run, example) -> EvaluationResult:\n",
        "    prediction = run.outputs.get(\"output\") or \"\"\n",
        "    required = example.outputs.get(\"must_mention\") or []\n",
        "    score = all(phrase in prediction for phrase in required)\n",
        "    return EvaluationResult(key=\"must_mention\", score=score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNtHORUh0jZY"
      },
      "source": [
        "#### ‚ùì Question #4:\n",
        "\n",
        "What are some ways you could improve this metric as-is?\n",
        "\n",
        "> NOTE: Alternatively you can suggest where gaps exist in this method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### ‚úÖ Answer:\n",
        "The current must_mention metric uses simple exact string matching which is very basic and problematic!\n",
        "Some ways to improve this metric are:\n",
        "\n",
        "1. **Semantic matching**: Use embeddings to catch related terms like \"LLM\" for \"Large Language Model\"\n",
        "2. **Partial credit**: Score based on percentage of terms found rather than all-or-nothing\n",
        "3. **Case insensitivity**: Normalize text to handle \"QLoRA\" vs \"qlora\"\n",
        "4. **Synonym handling**: Accept variations like \"ML\" for \"machine learning\"\n",
        "5. **Negation detection**: Fail when terms appear in negative contexts like \"does NOT use...\"\n",
        "6. **Stemming**: Match word variations like \"optimizer\", \"optimizers\", \"optimizing\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1RJr349zhv7"
      },
      "source": [
        "Task 3: Evaluating\n",
        "\n",
        "All that is left to do is evaluate our agent's response!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "efcf57067cf743d8b4ce059a61cbe02e",
            "53e33aae3b97490c82aec7bbb0d6ebba",
            "ad84e0e971d3455db2efe7dd0d1f803e",
            "72adef9b70dd48198b7322b6c5b113cf",
            "8a61d045ffd44ac58f3f13eb10044836",
            "041e22a9b5514e36bd4d1dac01d5d398",
            "886d762f2a7c421382efb5502c6d42a1",
            "ab91fd625bbd43afbf8c6398193a88d0",
            "716557ad09874dcb989d75f7c74424cd",
            "77d4c0ebaae045b58efc4f789c9a2360",
            "0d622ccc56264fac8fd7508dbdbe6e29"
          ]
        },
        "id": "p5TeCUUkuGld",
        "outputId": "2f7d62a2-e78d-447a-d07b-f9e4d500fb79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for experiment: 'Search Pipeline - Evaluation - 35a6-a6072390' at:\n",
            "https://smith.langchain.com/o/e60ce1c7-6903-49a6-9264-5c7bd472b609/datasets/497bf99c-1d53-414c-aa7c-b585f65a2156/compare?selectedSessions=c769d7c3-893c-47dc-a303-955c15cc0f83\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a2d36436db104168b83746e43fe32ad0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "experiment_results = client.evaluate(\n",
        "    agent_chain_with_formatting,\n",
        "    data=dataset_name,\n",
        "    evaluators=[must_mention],\n",
        "    experiment_prefix=f\"Search Pipeline - Evaluation - {uuid4().hex[0:4]}\",\n",
        "    metadata={\"version\": \"1.0.0\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        },
        "id": "eeEqU7s05Byu",
        "outputId": "78395075-a05d-4ebd-c798-ed968b935318"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<ExperimentResults Search Pipeline - Evaluation - 35a6-a6072390>"
            ],
            "text/plain": [
              "<ExperimentResults Search Pipeline - Evaluation - 35a6-a6072390>"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "experiment_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhTNe4kWrplB"
      },
      "source": [
        "## Part 2: LangGraph with Helpfulness:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1wKRddbIY_S"
      },
      "source": [
        "### Task 3: Adding Helpfulness Check and \"Loop\" Limits\n",
        "\n",
        "Now that we've done evaluation - let's see if we can add an extra step where we review the content we've generated to confirm if it fully answers the user's query!\n",
        "\n",
        "We're going to make a few key adjustments to account for this:\n",
        "\n",
        "1. We're going to add an artificial limit on how many \"loops\" the agent can go through - this will help us to avoid the potential situation where we never exit the loop.\n",
        "2. We'll add to our existing conditional edge to obtain the behaviour we desire."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npTYJ8ayR5B3"
      },
      "source": [
        "First, let's define our state again - we can check the length of the state object, so we don't need additional state for this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "-LQ84YhyJG0w"
      },
      "outputs": [],
      "source": [
        "class AgentState(TypedDict):\n",
        "  messages: Annotated[list, add_messages]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sD7EV0HqSQcb"
      },
      "source": [
        "Now we can set our graph up! This process will be almost entirely the same - with the inclusion of one additional node/conditional edge!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oajBwLkFVi1N"
      },
      "source": [
        "#### üèóÔ∏è Activity #5:\n",
        "\n",
        "Please write markdown for the following cells to explain what each is doing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### ‚úÖ Answer:\n",
        "We are redefiniting our AgentState, exactly as it was since we can just utilize the count of messages within it to count the number of iteratins - no additional variables needed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6r6XXA5FJbVf",
        "outputId": "ff713041-e498-4f0f-a875-a03502b87729"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x1336a2990>"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "graph_with_helpfulness_check = StateGraph(AgentState)\n",
        "\n",
        "graph_with_helpfulness_check.add_node(\"agent\", call_model)\n",
        "graph_with_helpfulness_check.add_node(\"action\", tool_node)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZ22o2mWVrfp"
      },
      "source": [
        "##### ‚úÖ Answer:\n",
        "\n",
        "We create a new StateGraph and add the same two fundamental nodes:\n",
        "- **agent node**: Calls the LLM to process queries and make decisions\n",
        "- **action node**: Executes tool calls (Tavily search, Arxiv queries)\n",
        "\n",
        "This gives us the same basic structure as our simple agent, but we'll enhance it with helpfulness checking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNWHwWxuRiLY",
        "outputId": "295f5a35-ceff-452a-ffb8-c52eada6a816"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x1336a2990>"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "graph_with_helpfulness_check.set_entry_point(\"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsXeF6xlaXOZ"
      },
      "source": [
        "##### ‚úÖ Answer:\n",
        "\n",
        "We set the agent node as the entry point, meaning all conversations start by having the LLM process the initial user query. This is the same pattern as our simple agent - the agent always gets first look at incoming messages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "z_Sq3A9SaV1O"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "def tool_call_or_helpful(state):\n",
        "  last_message = state[\"messages\"][-1]\n",
        "\n",
        "  if last_message.tool_calls:\n",
        "    return \"action\"\n",
        "\n",
        "  initial_query = state[\"messages\"][0]\n",
        "  final_response = state[\"messages\"][-1]\n",
        "\n",
        "  if len(state[\"messages\"]) > 10:\n",
        "    return \"END\"\n",
        "\n",
        "  prompt_template = \"\"\"\\\n",
        "  Given an initial query and a final response, determine if the final response is extremely helpful or not. Please indicate helpfulness with a 'Y' and unhelpfulness as an 'N'.\n",
        "\n",
        "  Initial Query:\n",
        "  {initial_query}\n",
        "\n",
        "  Final Response:\n",
        "  {final_response}\"\"\"\n",
        "\n",
        "  helpfullness_prompt_template = PromptTemplate.from_template(prompt_template)\n",
        "\n",
        "  helpfulness_check_model = ChatOpenAI(model=\"gpt-4.1-mini\")\n",
        "\n",
        "  helpfulness_chain = helpfullness_prompt_template | helpfulness_check_model | StrOutputParser()\n",
        "\n",
        "  helpfulness_response = helpfulness_chain.invoke({\"initial_query\" : initial_query.content, \"final_response\" : final_response.content})\n",
        "\n",
        "  if \"Y\" in helpfulness_response:\n",
        "    return \"end\"\n",
        "  else:\n",
        "    return \"continue\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fz1u9Vf4SHxJ"
      },
      "source": [
        "#### üèóÔ∏è Activity #4:\n",
        "\n",
        "Please write what is happening in our `tool_call_or_helpful` function!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### ‚úÖ Answer:\n",
        "\n",
        "This function replaces our simple `should_continue` with more sophisticated routing logic that:\n",
        "\n",
        "1. **Checks for tool calls**: If the agent wants to use tools, routes to \"action\"\n",
        "2. **Implements loop limits**: Prevents infinite cycles by checking message count (max 10)\n",
        "3. **Evaluates helpfulness**: Uses a secondary LLM to judge if the response adequately answers the original query\n",
        "   - This logic allows for continuation of the response isn't deemed relevant/appropriate enough!\n",
        "4. **Routes intelligently**: \n",
        "   - \"action\" = use tools\n",
        "   - \"continue\" = try again (response wasn't helpful enough)\n",
        "   - \"end\" = finished (response is satisfactory OR there are have been more than 10 iterations)\n",
        "\n",
        "This creates a more robust agent that can self-evaluate and retry when needed, or abort after looping the max number of times (10)!."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVTKnWMbP_8T",
        "outputId": "7f729b1f-311c-4084-ceaf-0da437900c85"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x1336a2990>"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "graph_with_helpfulness_check.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    tool_call_or_helpful,\n",
        "    {\n",
        "        \"continue\" : \"agent\",\n",
        "        \"action\" : \"action\",\n",
        "        \"end\" : END\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGDLEWOIVtK0"
      },
      "source": [
        "##### ‚úÖ Answer:\n",
        "\n",
        "We add conditional edges from the agent node using our enhanced logic function. The routing mapping handles three possible outcomes:\n",
        "\n",
        "- **\"continue\" ‚Üí \"agent\"**: Loops back to try generating a better response\n",
        "- **\"action\" ‚Üí \"action\"**: Routes to tool usage when needed\n",
        "- **\"end\" ‚Üí END**: Terminates when response is satisfactory\n",
        "\n",
        "This creates a more sophisticated flow that can retry and self-improve.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbDK2MbuREgU",
        "outputId": "21a64c20-27a1-4e0e-afde-a639abaa8b55"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x1336a2990>"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "graph_with_helpfulness_check.add_edge(\"action\", \"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSI8AOaEVvT-"
      },
      "source": [
        "##### ‚úÖ Answer:\n",
        "\n",
        "After any tool usage, we always route back to the agent node so it can:\n",
        "- Process the tool results\n",
        "- Decide whether to use additional tools or provide a final response\n",
        "- **If providing a final response**: Trigger helpfulness evaluation\n",
        "- **If needing more information**: Make another tool call\n",
        "\n",
        "This ensures the agent can iteratively gather information through multiple tool usage cycles before being evaluated on its final response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "oQldl8ERQ8lf"
      },
      "outputs": [],
      "source": [
        "agent_with_helpfulness_check = graph_with_helpfulness_check.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F67FGCMRVwGz"
      },
      "source": [
        "##### ‚úÖ Answer:\n",
        "\n",
        "We compile our enhanced graph into a runnable agent that now includes:\n",
        "\n",
        "- **Intelligent tool usage** (same as before - decides when to use Tavily/Arxiv tools)\n",
        "- **Loop limits** (stops after 10 message cycles to prevent infinite loops)\n",
        "- **Helpfulness evaluation** (uses a secondary LLM to judge if responses adequately answer the original question)\n",
        "- **Self-correction capability** (regenerates responses when the helpfulness evaluator deems them insufficient)\n",
        "\n",
        "This creates a more robust agent that can assess its own response quality and iteratively improve until it provides a satisfactory answer to the user's question or we hit the loop limit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3oo8E-PRK1T",
        "outputId": "f152dea8-96ad-4d29-d8b2-a064c96a8bd3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_dDY9Lki5laEBUGF0jXoxhzJl', 'function': {'arguments': '{\"query\": \"LoRA machine learning\"}', 'name': 'arxiv'}, 'type': 'function'}, {'id': 'call_hYncSrlwAkq1siVPYFqkyUli', 'function': {'arguments': '{\"query\": \"Tim Dettmers\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}, {'id': 'call_0uOjd0O37JPPUpBNoH6pxVrj', 'function': {'arguments': '{\"query\": \"Attention mechanism machine learning\"}', 'name': 'arxiv'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 71, 'prompt_tokens': 177, 'total_tokens': 248, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a288987b44', 'id': 'chatcmpl-BsplyWwObDzvMIULFclhcl762uGN7', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--7f19a30c-1f79-42dc-afbb-eb9f06108772-0', tool_calls=[{'name': 'arxiv', 'args': {'query': 'LoRA machine learning'}, 'id': 'call_dDY9Lki5laEBUGF0jXoxhzJl', 'type': 'tool_call'}, {'name': 'tavily_search_results_json', 'args': {'query': 'Tim Dettmers'}, 'id': 'call_hYncSrlwAkq1siVPYFqkyUli', 'type': 'tool_call'}, {'name': 'arxiv', 'args': {'query': 'Attention mechanism machine learning'}, 'id': 'call_0uOjd0O37JPPUpBNoH6pxVrj', 'type': 'tool_call'}], usage_metadata={'input_tokens': 177, 'output_tokens': 71, 'total_tokens': 248, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'action'\n",
            "[ToolMessage(content=\"Published: 2024-10-28\\nTitle: KD-LoRA: A Hybrid Approach to Efficient Fine-Tuning with LoRA and Knowledge Distillation\\nAuthors: Rambod Azimi, Rishav Rishav, Marek Teichmann, Samira Ebrahimi Kahou\\nSummary: Large language models (LLMs) have demonstrated remarkable performance across\\nvarious downstream tasks. However, the high computational and memory\\nrequirements of LLMs are a major bottleneck. To address this,\\nparameter-efficient fine-tuning (PEFT) methods such as low-rank adaptation\\n(LoRA) have been proposed to reduce computational costs while ensuring minimal\\nloss in performance. Additionally, knowledge distillation (KD) has been a\\npopular choice for obtaining compact student models from teacher models. In\\nthis work, we present KD-LoRA, a novel fine-tuning method that combines LoRA\\nwith KD. Our results demonstrate that KD-LoRA achieves performance comparable\\nto full fine-tuning (FFT) and LoRA while significantly reducing resource\\nrequirements. Specifically, KD-LoRA retains 98% of LoRA's performance on the\\nGLUE benchmark, while being 40% more compact. Additionally, KD-LoRA reduces GPU\\nmemory usage by 30% compared to LoRA, while decreasing inference time by 30%\\ncompared to both FFT and LoRA. We evaluate KD-LoRA across three encoder-only\\nmodels: BERT, RoBERTa, and DeBERTaV3. Code is available at\\nhttps://github.com/rambodazimi/KD-LoRA.\\n\\nPublished: 2024-04-07\\nTitle: A Note on LoRA\\nAuthors: Vlad Fomenko, Han Yu, Jongho Lee, Stanley Hsieh, Weizhu Chen\\nSummary: LoRA (Low-Rank Adaptation) has emerged as a preferred method for efficiently\\nadapting Large Language Models (LLMs) with remarkable simplicity and efficacy.\\nThis note extends the original LoRA paper by offering new perspectives that\\nwere not initially discussed and presents a series of insights for deploying\\nLoRA at scale. Without introducing new experiments, we aim to improve the\\nunderstanding and application of LoRA.\\n\\nPublished: 2024-06-18\\nTitle: LoRA-drop: Efficient LoRA Parameter Pruning based on Output Evaluation\\nAuthors: Hongyun Zhou, Xiangyu Lu, Wang Xu, Conghui Zhu, Tiejun Zhao, Muyun Yang\\nSummary: Low-Rank Adaptation (LoRA) is currently the most commonly used\\nParameter-efficient fine-tuning (PEFT) method, it introduces auxiliary\\nparameters for each layer to fine-tune the pre-trained model under limited\\ncomputing resources. However, it still faces resource consumption challenges\\nduring training when scaling up to larger models. Most previous studies have\\ntackled this issue by using pruning techniques, which involve removing LoRA\\nparameters deemed unimportant. Nonetheless, these efforts only analyze LoRA\\nparameter features to evaluate their importance, such as parameter count, size,\\nand gradient. In fact, the output of LoRA (product of LoRA parameter and hidden\\nstate), directly impacts the final results. Preliminary experiments indicate\\nthat a fraction of LoRA elements possesses significantly high output values,\\nsubstantially influencing the layer output. Motivated by the observation, we\\npropose LoRA-drop. Concretely, LoRA-drop evaluates the importance of LoRA based\\non the LoRA output. Then we retain LoRA for important layers and the other\\nlayers share the same LoRA. We conduct abundant experiments with models of\\ndifferent scales on NLU and NLG tasks. Results demonstrate that LoRA-drop can\\nachieve performance comparable to full fine-tuning and LoRA, while retaining\\n50\\\\% of the LoRA parameters on average.\", name='arxiv', id='1872c2da-d0f2-4199-84ef-447d9297deaf', tool_call_id='call_dDY9Lki5laEBUGF0jXoxhzJl'), ToolMessage(content='[{\"title\": \"Tim Dettmers - AI2050 - Schmidt Sciences\", \"url\": \"https://ai2050.schmidtsciences.org/fellow/tim-dettmers/\", \"content\": \"Tim Dettmers is an Assistant Professor at Carnegie Mellon University and a Research Scientist at the Allen Institute for AI, and his research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning. He has won oral, spotlight, and best paper awards at conferences [...] such as ICLR and NeurIPS. He created the bitsandbytes library for efficient deep learning, which is growing at 2.2 million installations per month, and received Google Open Source and PyTorch Foundation awards. [...] AI models like ChatGPT work well for general use but fail in specialized expert domains, such as the medical sciences. To make AI models work in expert domains, one must adapt them, which is costly and requires significant AI expertise. This project overcomes these cost and expertise barriers through two new approaches: (1) use AI models themselves to perform the AI model adaptation process automatically; (2) make the adaptation process cheap so it can be run on regular consumer hardware. With\", \"score\": 0.9245858}, {\"title\": \"CSE Faculty Candidate Seminar - Tim Dettmers\", \"url\": \"https://cse.gatech.edu/events/2024/02/20/cse-faculty-candidate-seminar-tim-dettmers\", \"content\": \"Bio:Tim Dettmers‚Äôs research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning. These methods enable many more people to use, adapt, or train foundation models without affecting the quality of AI predictions or generations. He is a PhD candidate at the [...] News and Events\\\\n       News\\\\n       Upcoming Events\\\\n       Calendar\\\\n       CSE Biweekly Roundup\\\\n       Analyzer\\\\n       2024 Annual Brief.pdf)\\\\n\\\\n\\\\n\\\\nSearch\\\\n------\\\\n\\\\nSearch \\\\n\\\\nBreadcrumb\\\\n----------\\\\n\\\\n1.   Home\\\\n2.   Events\\\\n\\\\nUpcoming Events\\\\n---------------\\\\n\\\\nCSE Faculty Candidate Seminar - Tim Dettmers\\\\n============================================\\\\n\\\\nImage 2: Tim Dettmers.png\\\\n\\\\nName:Tim Dettmers, Ph.D. student at University of Washington\\\\n\\\\nDate:Tuesday, February 20, 2024 at 11:00 am [...] University of Washington and has won oral, spotlight, and best paper awards at conferences such as ICLR and NeurIPS. He created the bitsandbytes library for efficient deep learning, which is growing at 1.4 million installations per month and received Google Open Source and PyTorch Foundation awards.\", \"score\": 0.70531887}, {\"title\": \"Tim Dettmers - OpenReview\", \"url\": \"https://openreview.net/profile?id=~Tim_Dettmers2\", \"content\": \"Toggle navigationOpenReview.net\\\\n\\\\n   Login\\\\n\\\\n√ó\\\\n\\\\n√ó\\\\n### BibTeX Record\\\\n\\\\n_Click anywhere on the box above to highlight complete record_\\\\n\\\\nDone\\\\n\\\\nTim Dettmers\\\\n============\\\\n\\\\n### Assistant Professor, Machine learning; Computer Science, Carnegie Mellon University\\\\n\\\\n### Researcher, Allen Institute for Artificial Intelligence\\\\n\\\\n    Joined September 2019\\\\n\\\\n#### Names\\\\n\\\\nTim Dettmers(Preferred)\\\\n\\\\n   Suggest Name\\\\n\\\\n#### Emails\\\\n\\\\n@gmail.com(Confirmed)\\\\n\\\\n, \\\\n\\\\n@cs.washington.edu(Confirmed)\\\\n\\\\n, \\\\n\\\\n@fb.com(Confirmed)\\\\n\\\\n, [...] @allenai.org(Confirmed)\\\\n\\\\n, \\\\n\\\\n@cmu.edu(Confirmed)\\\\n\\\\n   Suggest Email\\\\n\\\\n#### Personal Links\\\\n\\\\nHomepage\\\\n\\\\nGoogle Scholar\\\\n\\\\nDBLP\\\\n\\\\n   Suggest URL\\\\n\\\\n#### Career & Education History\\\\n\\\\nAssistant Professor\\\\n\\\\nMachine learning; Computer Science, Carnegie Mellon University(cmu.edu)\\\\n\\\\n_2025 ‚Äì Present_\\\\n\\\\nResearcher\\\\n\\\\nAllen Institute for Artificial Intelligence(allenai.org)\\\\n\\\\n_2024 ‚Äì Present_\\\\n\\\\nPhD student\\\\n\\\\nComputer Science, University of Washington(cs.washington.edu)\\\\n\\\\n_2018 ‚Äì 2024_\\\\n\\\\n   Suggest Position\", \"score\": 0.70531887}, {\"title\": \"About Me - Tim Dettmers\", \"url\": \"https://timdettmers.com/about/\", \"content\": \"Research Interests  \\\\nPublications  \\\\nAwards & Honors  \\\\nService\\\\n\\\\nGoogle Scholar\\\\n\\\\nfirstname.lastname@gmail.com\\\\n\\\\nImage 1I am a research scientist at the Allen Institute for Artificial Intelligence (Ai2) and an incoming Assistant Professor at Carnegie Mellon University (CMU). I am the creator and maintainer of bitsandbytes. [...] I have a PhD from University of Washington advised by Luke Zettlemoyer working on efficient deep learning at the intersection between machine learning, natural language processing, and computer systems with a focus on quantization and sparsity. My main research goal is to empower everyone to make AI their own. I do this by making large models accessible through my research (QLoRA, LLM.int8(), k-bit inference scaling laws, Petals, SWARM) and by developing software that makes it easy to use my [...] About Me ‚Äî Tim Dettmers\\\\n===============  \\\\n\\\\nSkip links\\\\n----------\\\\n\\\\n   Skip to primary navigation\\\\n   Skip to content\\\\n   Skip to primary sidebar\\\\n\\\\nTim Dettmers\\\\n\\\\nMaking deep learning accessible.\\\\n\\\\nHeader Right\\\\n------------\\\\n\\\\n### Blog Posts Topics\\\\n\\\\n   Academia (4)\\\\n       PhD Life (3)\\\\n   Deep Learning (7)\\\\n   Hardware (8)\\\\n   Science (4)\\\\n       Neuroscience (1)\\\\n\\\\nMain navigation\\\\n---------------\\\\n\\\\n   Blog\\\\n       Deep Learning\\\\n       Hardware\\\\n       Neuroscience\\\\n   Publications\\\\n   About Me\\\\n\\\\nAbout Me\\\\n========\", \"score\": 0.68318754}, {\"title\": \"Tim Dettmers | Carnegie Mellon University Computer Science ...\", \"url\": \"https://csd.cmu.edu/people/faculty/tim-dettmers\", \"content\": \"X\\\\n\\\\nBreadcrumb\\\\n----------\\\\n\\\\n1.  Home\\\\n2.  People\\\\n3.  Faculty\\\\n4.  Tim Dettmers\\\\n\\\\nTim Dettmers\\\\n============\\\\n\\\\nImage 2: Tim DettmersAssistant Professor\\\\n\\\\nWebsite\\\\n\\\\nGoogle Scholars Link\\\\n\\\\nEmail dettmers@cmu.edu\\\\n\\\\nDepartment  \\\\nMachine Learning Department  \\\\nComputer Science Department\\\\n\\\\nComputer Science Department\\\\n---------------------------\\\\n\\\\nCarnegie Mellon University\\\\n\\\\n5000 Forbes Avenue\\\\n\\\\nPittsburgh, PA 15213\\\\n\\\\nFax: 412-268-5576\\\\n\\\\n            \\\\n\\\\nImage 3: Carnegie Mellon University School of Computer Science [...] About\\\\n    \\\\n    ### Back to Main Menu\\\\n    \\\\n    ### About Main page\\\\n    \\\\n       About  \\\\n        Related links\\\\n           Events\\\\n           News\\\\n           Key Contacts\\\\n           History\\\\n           Sitemap\\\\n       Employment  \\\\n        Related links\\\\n           Faculty Hiring\\\\n           Staff Hiring\\\\n       Marketing & Communications  \\\\n        Related links\\\\n           SCS Marketing & Communications\\\\n           Partnerships\\\\n           Employer Recruiting\\\\n           CMU Marketing & Communications\", \"score\": 0.6729558}]', name='tavily_search_results_json', id='c325693d-35b8-4b4a-8507-582e4a23eeaf', tool_call_id='call_hYncSrlwAkq1siVPYFqkyUli', artifact={'query': 'Tim Dettmers', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://ai2050.schmidtsciences.org/fellow/tim-dettmers/', 'title': 'Tim Dettmers - AI2050 - Schmidt Sciences', 'content': 'Tim Dettmers is an Assistant Professor at Carnegie Mellon University and a Research Scientist at the Allen Institute for AI, and his research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning. He has won oral, spotlight, and best paper awards at conferences [...] such as ICLR and NeurIPS. He created the bitsandbytes library for efficient deep learning, which is growing at 2.2 million installations per month, and received Google Open Source and PyTorch Foundation awards. [...] AI models like ChatGPT work well for general use but fail in specialized expert domains, such as the medical sciences. To make AI models work in expert domains, one must adapt them, which is costly and requires significant AI expertise. This project overcomes these cost and expertise barriers through two new approaches: (1) use AI models themselves to perform the AI model adaptation process automatically; (2) make the adaptation process cheap so it can be run on regular consumer hardware. With', 'score': 0.9245858, 'raw_content': None}, {'url': 'https://cse.gatech.edu/events/2024/02/20/cse-faculty-candidate-seminar-tim-dettmers', 'title': 'CSE Faculty Candidate Seminar - Tim Dettmers', 'content': 'Bio:Tim Dettmers‚Äôs research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning. These methods enable many more people to use, adapt, or train foundation models without affecting the quality of AI predictions or generations. He is a PhD candidate at the [...] News and Events\\n       News\\n       Upcoming Events\\n       Calendar\\n       CSE Biweekly Roundup\\n       Analyzer\\n       2024 Annual Brief.pdf)\\n\\n\\n\\nSearch\\n------\\n\\nSearch \\n\\nBreadcrumb\\n----------\\n\\n1.   Home\\n2.   Events\\n\\nUpcoming Events\\n---------------\\n\\nCSE Faculty Candidate Seminar - Tim Dettmers\\n============================================\\n\\nImage 2: Tim Dettmers.png\\n\\nName:Tim Dettmers, Ph.D. student at University of Washington\\n\\nDate:Tuesday, February 20, 2024 at 11:00 am [...] University of Washington and has won oral, spotlight, and best paper awards at conferences such as ICLR and NeurIPS. He created the bitsandbytes library for efficient deep learning, which is growing at 1.4 million installations per month and received Google Open Source and PyTorch Foundation awards.', 'score': 0.70531887, 'raw_content': None}, {'url': 'https://openreview.net/profile?id=~Tim_Dettmers2', 'title': 'Tim Dettmers - OpenReview', 'content': 'Toggle navigationOpenReview.net\\n\\n   Login\\n\\n√ó\\n\\n√ó\\n### BibTeX Record\\n\\n_Click anywhere on the box above to highlight complete record_\\n\\nDone\\n\\nTim Dettmers\\n============\\n\\n### Assistant Professor, Machine learning; Computer Science, Carnegie Mellon University\\n\\n### Researcher, Allen Institute for Artificial Intelligence\\n\\n    Joined September 2019\\n\\n#### Names\\n\\nTim Dettmers(Preferred)\\n\\n   Suggest Name\\n\\n#### Emails\\n\\n@gmail.com(Confirmed)\\n\\n, \\n\\n@cs.washington.edu(Confirmed)\\n\\n, \\n\\n@fb.com(Confirmed)\\n\\n, [...] @allenai.org(Confirmed)\\n\\n, \\n\\n@cmu.edu(Confirmed)\\n\\n   Suggest Email\\n\\n#### Personal Links\\n\\nHomepage\\n\\nGoogle Scholar\\n\\nDBLP\\n\\n   Suggest URL\\n\\n#### Career & Education History\\n\\nAssistant Professor\\n\\nMachine learning; Computer Science, Carnegie Mellon University(cmu.edu)\\n\\n_2025 ‚Äì Present_\\n\\nResearcher\\n\\nAllen Institute for Artificial Intelligence(allenai.org)\\n\\n_2024 ‚Äì Present_\\n\\nPhD student\\n\\nComputer Science, University of Washington(cs.washington.edu)\\n\\n_2018 ‚Äì 2024_\\n\\n   Suggest Position', 'score': 0.70531887, 'raw_content': None}, {'url': 'https://timdettmers.com/about/', 'title': 'About Me - Tim Dettmers', 'content': 'Research Interests  \\nPublications  \\nAwards & Honors  \\nService\\n\\nGoogle Scholar\\n\\nfirstname.lastname@gmail.com\\n\\nImage 1I am a research scientist at the Allen Institute for Artificial Intelligence (Ai2) and an incoming Assistant Professor at Carnegie Mellon University (CMU). I am the creator and maintainer of bitsandbytes. [...] I have a PhD from University of Washington advised by Luke Zettlemoyer working on efficient deep learning at the intersection between machine learning, natural language processing, and computer systems with a focus on quantization and sparsity. My main research goal is to empower everyone to make AI their own. I do this by making large models accessible through my research (QLoRA, LLM.int8(), k-bit inference scaling laws, Petals, SWARM) and by developing software that makes it easy to use my [...] About Me ‚Äî Tim Dettmers\\n===============  \\n\\nSkip links\\n----------\\n\\n   Skip to primary navigation\\n   Skip to content\\n   Skip to primary sidebar\\n\\nTim Dettmers\\n\\nMaking deep learning accessible.\\n\\nHeader Right\\n------------\\n\\n### Blog Posts Topics\\n\\n   Academia (4)\\n       PhD Life (3)\\n   Deep Learning (7)\\n   Hardware (8)\\n   Science (4)\\n       Neuroscience (1)\\n\\nMain navigation\\n---------------\\n\\n   Blog\\n       Deep Learning\\n       Hardware\\n       Neuroscience\\n   Publications\\n   About Me\\n\\nAbout Me\\n========', 'score': 0.68318754, 'raw_content': None}, {'url': 'https://csd.cmu.edu/people/faculty/tim-dettmers', 'title': 'Tim Dettmers | Carnegie Mellon University Computer Science ...', 'content': 'X\\n\\nBreadcrumb\\n----------\\n\\n1.  Home\\n2.  People\\n3.  Faculty\\n4.  Tim Dettmers\\n\\nTim Dettmers\\n============\\n\\nImage 2: Tim DettmersAssistant Professor\\n\\nWebsite\\n\\nGoogle Scholars Link\\n\\nEmail dettmers@cmu.edu\\n\\nDepartment  \\nMachine Learning Department  \\nComputer Science Department\\n\\nComputer Science Department\\n---------------------------\\n\\nCarnegie Mellon University\\n\\n5000 Forbes Avenue\\n\\nPittsburgh, PA 15213\\n\\nFax: 412-268-5576\\n\\n            \\n\\nImage 3: Carnegie Mellon University School of Computer Science [...] About\\n    \\n    ### Back to Main Menu\\n    \\n    ### About Main page\\n    \\n       About  \\n        Related links\\n           Events\\n           News\\n           Key Contacts\\n           History\\n           Sitemap\\n       Employment  \\n        Related links\\n           Faculty Hiring\\n           Staff Hiring\\n       Marketing & Communications  \\n        Related links\\n           SCS Marketing & Communications\\n           Partnerships\\n           Employer Recruiting\\n           CMU Marketing & Communications', 'score': 0.6729558, 'raw_content': None}], 'response_time': 1.88}), ToolMessage(content='Published: 2022-03-27\\nTitle: A General Survey on Attention Mechanisms in Deep Learning\\nAuthors: Gianni Brauwers, Flavius Frasincar\\nSummary: Attention is an important mechanism that can be employed for a variety of\\ndeep learning models across many different domains and tasks. This survey\\nprovides an overview of the most important attention mechanisms proposed in the\\nliterature. The various attention mechanisms are explained by means of a\\nframework consisting of a general attention model, uniform notation, and a\\ncomprehensive taxonomy of attention mechanisms. Furthermore, the various\\nmeasures for evaluating attention models are reviewed, and methods to\\ncharacterize the structure of attention models based on the proposed framework\\nare discussed. Last, future work in the field of attention models is\\nconsidered.\\n\\nPublished: 2022-07-04\\nTitle: Attention mechanisms for physiological signal deep learning: which attention should we take?\\nAuthors: Seong-A Park, Hyung-Chul Lee, Chul-Woo Jung, Hyun-Lim Yang\\nSummary: Attention mechanisms are widely used to dramatically improve deep learning\\nmodel performance in various fields. However, their general ability to improve\\nthe performance of physiological signal deep learning model is immature. In\\nthis study, we experimentally analyze four attention mechanisms (e.g.,\\nsqueeze-and-excitation, non-local, convolutional block attention module, and\\nmulti-head self-attention) and three convolutional neural network (CNN)\\narchitectures (e.g., VGG, ResNet, and Inception) for two representative\\nphysiological signal prediction tasks: the classification for predicting\\nhypotension and the regression for predicting cardiac output (CO). We evaluated\\nmultiple combinations for performance and convergence of physiological signal\\ndeep learning model. Accordingly, the CNN models with the spatial attention\\nmechanism showed the best performance in the classification problem, whereas\\nthe channel attention mechanism achieved the lowest error in the regression\\nproblem. Moreover, the performance and convergence of the CNN models with\\nattention mechanisms were better than stand-alone self-attention models in both\\nproblems. Hence, we verified that convolutional operation and attention\\nmechanisms are complementary and provide faster convergence time, despite the\\nstand-alone self-attention models requiring fewer parameters.\\n\\nPublished: 2018-10-17\\nTitle: An Analysis of Attention Mechanisms: The Case of Word Sense Disambiguation in Neural Machine Translation\\nAuthors: Gongbo Tang, Rico Sennrich, Joakim Nivre\\nSummary: Recent work has shown that the encoder-decoder attention mechanisms in neural\\nmachine translation (NMT) are different from the word alignment in statistical\\nmachine translation. In this paper, we focus on analyzing encoder-decoder\\nattention mechanisms, in the case of word sense disambiguation (WSD) in NMT\\nmodels. We hypothesize that attention mechanisms pay more attention to context\\ntokens when translating ambiguous words. We explore the attention distribution\\npatterns when translating ambiguous nouns. Counter-intuitively, we find that\\nattention mechanisms are likely to distribute more attention to the ambiguous\\nnoun itself rather than context tokens, in comparison to other nouns. We\\nconclude that attention mechanism is not the main mechanism used by NMT models\\nto incorporate contextual information for WSD. The experimental results suggest\\nthat NMT models learn to encode contextual information necessary for WSD in the\\nencoder hidden states. For the attention mechanism in Transformer models, we\\nreveal that the first few layers gradually learn to \"align\" source and target\\ntokens and the last few layers learn to extract features from the related but\\nunaligned context tokens.', name='arxiv', id='7aff640c-e755-40e7-8114-8a6d751a0b13', tool_call_id='call_0uOjd0O37JPPUpBNoH6pxVrj')]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='### LoRA in Machine Learning\\nLoRA, or Low-Rank Adaptation, is a parameter-efficient fine-tuning method used in machine learning, particularly for adapting large language models (LLMs). It aims to reduce computational costs while maintaining performance. LoRA introduces auxiliary parameters for each layer to fine-tune pre-trained models under limited computing resources. Recent advancements like KD-LoRA combine LoRA with knowledge distillation to further reduce resource requirements while retaining high performance. LoRA-drop is another technique that prunes LoRA parameters based on output evaluation to optimize resource usage.\\n\\n### Tim Dettmers\\nTim Dettmers is an Assistant Professor at Carnegie Mellon University and a Research Scientist at the Allen Institute for AI. His research focuses on making foundation models, such as ChatGPT, more accessible by reducing their resource requirements. He develops novel compression and networking algorithms to enable memory-efficient, fast, and cost-effective deep learning. Dettmers is known for creating the bitsandbytes library for efficient deep learning and has received awards from Google Open Source and the PyTorch Foundation.\\n\\n### Attention Mechanism in Machine Learning\\nAttention mechanisms are crucial in deep learning models across various domains. They allow models to focus on specific parts of the input data, improving performance in tasks like neural machine translation and physiological signal prediction. Different types of attention mechanisms, such as multi-head self-attention and convolutional block attention modules, are used to enhance model performance. Attention mechanisms are complementary to convolutional operations, providing faster convergence and improved performance in deep learning models.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 308, 'prompt_tokens': 3444, 'total_tokens': 3752, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a288987b44', 'id': 'chatcmpl-Bspm5A9bPF3QtsWq8D6qEGwm9KrvJ', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--750f3185-c4d2-42a5-8d33-904b3742ad48-0', usage_metadata={'input_tokens': 3444, 'output_tokens': 308, 'total_tokens': 3752, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "inputs = {\"messages\" : [HumanMessage(content=\"Related to machine learning, what is LoRA? Also, who is Tim Dettmers? Also, what is Attention?\")]}\n",
        "\n",
        "async for chunk in agent_with_helpfulness_check.astream(inputs, stream_mode=\"updates\"):\n",
        "    for node, values in chunk.items():\n",
        "        print(f\"Receiving update from node: '{node}'\")\n",
        "        print(values[\"messages\"])\n",
        "        print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVmZPs6lnpsM"
      },
      "source": [
        "### Task 4: LangGraph for the \"Patterns\" of GenAI\n",
        "\n",
        "Let's ask our system about the 4 patterns of Generative AI:\n",
        "\n",
        "1. Prompt Engineering\n",
        "2. RAG\n",
        "3. Fine-tuning\n",
        "4. Agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "ZoLl7GlXoae-"
      },
      "outputs": [],
      "source": [
        "patterns = [\"prompt engineering\", \"RAG\", \"fine-tuning\", \"LLM-based agents\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zkh0YJuCp3Zl",
        "outputId": "d847426e-71b3-47e6-b1ae-351a78d68d1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**Prompt Engineering: Definition and Overview**\n",
            "\n",
            "Prompt engineering is the process of designing and refining prompts‚Äîquestions or instructions‚Äîto elicit specific responses from AI models, particularly large language models (LLMs) like OpenAI's GPT, Google's Gemini, and Anthropic's Claude. It involves crafting well-structured, precise inputs to guide the outputs of AI models. This practice is crucial for ensuring effective human-AI communication, as it helps bridge the gap between human intent and machine understanding. The right prompt can significantly influence whether a model accurately understands and responds to a request.\n",
            "\n",
            "**History and Emergence of Prompt Engineering**\n",
            "\n",
            "Prompt engineering has evolved alongside advancements in AI and natural language processing (NLP). Initially, it involved simple, structured inputs to guide models in performing tasks. The introduction of attention mechanisms in 2015 revolutionized language understanding, leading to advancements in controllability and context-awareness. The release of ChatGPT in 2022 marked a significant point where prompt engineering became recognized as an important business skill. The field has continued to grow, with developments in contextual prompting, transfer learning, and ethical considerations shaping its future.\n",
            "\n",
            "\n",
            "\n",
            "Retrieval-Augmented Generation (RAG) is a technology that combines information retrieval (IR) and natural language generation (NLG) to enhance the capabilities of AI systems, particularly in knowledge-intensive tasks. The concept of RAG has its roots in the early developments of information retrieval systems in the 1950s and 1960s, with foundational work by researchers like Hans Peter Luhn and Gerald Salton. These early systems laid the groundwork for the integration of retrieval and generation technologies.\n",
            "\n",
            "The term \"RAG\" was formally introduced in 2020 by researchers at Meta, including Douwe Kiela, Patrick Lewis, and others, in their paper \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.\" This marked a significant milestone in the development of RAG, as it provided a new architecture for applying this concept within modern AI frameworks.\n",
            "\n",
            "RAG technology gained widespread attention with the emergence of question-answering systems like Ask Jeeves in the mid-1990s and IBM's Watson in 2011. These systems demonstrated the potential of combining retrieval and generation to provide accurate and contextually relevant responses.\n",
            "\n",
            "Recent advancements in RAG have focused on enhancing the accuracy and efficiency of these systems by integrating advanced retrievers, large language models (LLMs), and other complementary technologies. Research continues to explore new frameworks, such as Modular RAG and AutoRAG, to optimize the performance of RAG systems across different datasets and application scenarios.\n",
            "\n",
            "\n",
            "\n",
            "Fine-tuning in machine learning is a process where a pretrained model is further trained on a smaller, targeted dataset to adapt it for more specialized use cases. This technique is part of transfer learning and involves making small adjustments to a model's parameters to improve its performance on a specific task. Fine-tuning can be applied to the entire model or just a subset of its layers, with the rest being \"frozen\" during training. This approach is widely used in various applications, such as improving product recommendation engines in e-commerce by training on user interaction data.\n",
            "\n",
            "The concept of fine-tuning has been around for some time, but it gained significant attention with the rise of deep learning and transfer learning techniques. The historical development of fine-tuning is closely tied to the evolution of large language models (LLMs) and other AI systems, which have increasingly relied on fine-tuning to enhance their capabilities for specific tasks.\n",
            "\n",
            "For more detailed insights, you can explore the following resources:\n",
            "- [TechTarget on Fine-Tuning](https://www.techtarget.com/searchenterpriseai/definition/fine-tuning)\n",
            "- [Wikipedia on Fine-Tuning in Deep Learning](https://en.wikipedia.org/wiki/Fine-tuning_(deep_learning))\n",
            "- [The Ultimate Guide to Fine-Tuning LLMs](https://arxiv.org/html/2408.13296v1)\n",
            "\n",
            "These resources provide a comprehensive overview of fine-tuning, its methodologies, and its applications in modern AI systems.\n",
            "\n",
            "\n",
            "\n",
            "LLM-based agents, or Large Language Model-based agents, are AI systems that leverage large language models, tools, and memory to perform tasks, make decisions, and interact with users or other systems autonomously. These agents are designed to process and understand language based on vast amounts of data they have been trained on, allowing them to perform complex tasks and interact with humans and other systems effectively.\n",
            "\n",
            "The concept of LLM-based agents has been gaining traction in recent years, with significant developments and discussions occurring around 2023 and 2024. For instance, a paper titled \"TrustAgent: Towards Safe and Trustworthy LLM-based Agents\" was published in 2024, indicating ongoing research and interest in this area. Additionally, articles and guides discussing LLM agents have been published as recently as 2025, highlighting their evolving nature and applications.\n",
            "\n",
            "The emergence of LLM-based agents is part of a broader trend in AI towards creating more autonomous and intelligent systems capable of performing a wide range of tasks without direct human intervention. This trend is supported by advancements in AI research, particularly in the development of large language models and reinforcement learning techniques.\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for pattern in patterns:\n",
        "  what_is_string = f\"What is {pattern} and when did it break onto the scene??\"\n",
        "  inputs = {\"messages\" : [HumanMessage(content=what_is_string)]}\n",
        "  messages = agent_with_helpfulness_check.invoke(inputs)\n",
        "  print(messages[\"messages\"][-1].content)\n",
        "  print(\"\\n\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
